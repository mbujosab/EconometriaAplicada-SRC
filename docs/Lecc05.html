<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es-es" xml:lang="es-es">
<head>
<!-- 2024-09-27 vie 17:38 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Econometría Aplicada. Lección 5</title>
<meta name="author" content="Marcos Bujosa" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Econometría Aplicada. Lección 5</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org699ffbc">Carga de algunas librerías de R</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc14eef1">1. Series temporales vs datos de sección cruzada</a>
<ul>
<li><a href="#org8bb2aeb">1.1. Correlación serial vs muestreo aleatorio simple</a></li>
<li><a href="#org3daa50f">1.2. Simplificación del escenario</a></li>
</ul>
</li>
<li><a href="#org635beae">2. Procesos estocásticos de segundo orden</a>
<ul>
<li><a href="#org78b5d6c">2.1. Un poco de geometría</a></li>
<li><a href="#orgb0f529e">2.2. Primeros momentos de procesos estocásticos de segundo orden</a></li>
<li><a href="#org81a45c2">2.3. Procesos estocásticos (débilmente) estacionarios y la ACF</a></li>
</ul>
</li>
<li><a href="#org3c75a39">3. Notación: convolución y el operador retardo</a></li>
<li><a href="#org0cc7da8">4. Ejemplos de procesos (débilmente) estacionarios</a>
<ul>
<li><a href="#orgb0bb725">4.1. Proceso de ruido blanco</a></li>
<li><a href="#org71db0d9">4.2. Procesos lineales</a>
<ul>
<li><a href="#org74f4fb6">4.2.1. Media móvil infinita. MA(\(\infty\))</a></li>
<li><a href="#org7f39499">4.2.2. Proceso de media móvil de orden \(q\). MA(\(q\))</a></li>
<li><a href="#orgef8ae33">4.2.3. Proceso autorregresivo de orden \(p\). AR(\(p\))</a></li>
<li><a href="#org9f6caaf">4.2.4. Proceso autorregresivo de media móvil. ARMA(\(p,q\))</a></li>
<li><a href="#orgd5e31dd">4.2.5. Proceso autorregresivo de media móvil con media no nula</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org48d78cf">5. Primeros momentos de procesos lineales causales</a>
<ul>
<li><a href="#org8d85be3">5.1. Esperanza y autocovarianzas de un proceso lineal causal</a></li>
<li><a href="#orgab0395e">5.2. Covarianza cruzada entre dos procesos lineales causales</a></li>
<li><a href="#org392fab2">5.3. Las Ecuaciones de Yule-Walker para un AR(\(p\)) estacionario</a></li>
<li><a href="#org749e848">5.4. Función de autocovarianzas para un ARMA(\(p,q\))</a></div>
</div>
<div class="abstract" id="orgb810239">
<p>
Esta lección veremos las dificultades que ocasiona la correlación
serial y algunos tipos de procesos débilmente estacionarios que nos
permitirán lidiar con ella. En particular veremos los procesos
lineales, su valor esperado y su función de autocovarianzas, la
función de covarianzas cruzadas entre dos procesos lineales, y las
ecuaciones de Yule-Walker.
</p>

</div>

<ul class="org-ul">
<li><a href="https://mbujosab.github.io/EconometriaAplicada-SRC/Lecc05.html">lección en html</a></li>
<li><a href="https://mybinder.org/v2/gh/mbujosab/EconometriaAplicada-SRC/HEAD?labpath=Lecc05.ipynb">lección en mybinder</a></li>
</ul>
<div id="outline-container-org699ffbc" class="outline-4">
<h4 id="org699ffbc">Carga de algunas librerías de R</h4>
<div class="outline-text-4" id="text-org699ffbc">
<p>
Primero cargamos la librería <code>tfarima</code> (Repositorio Cran:
<a href="https://cran.r-project.org/web/packages/tfarima/index.html">https://cran.r-project.org/web/packages/tfarima/index.html</a>;
repositorio GitHub: <a href="https://github.com/gallegoj/tfarima">https://github.com/gallegoj/tfarima</a>)
</p>
<div class="org-src-container">
<pre class="src src-jupyter-R"><span style="color: #D0372D;">library</span>(tfarima)      <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">librer&#237;a de Jos&#233; Luis Gallego para Time Series</span>
<span style="color: #D0372D;">library</span>(readr)        <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">para leer ficheros CSV</span>
<span style="color: #D0372D;">library</span>(ggplot2)      <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">para el scatterplot (alternaticamente library(tidyverse))</span>
<span style="color: #D0372D;">library</span>(ggfortify)    <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">para pintar series temporales</span>
<span style="color: #D0372D;">library</span>(jtools)       <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">para representaci&#243;n resultados estimaci&#243;n</span>
<span style="color: #D0372D;">library</span>(zoo)          <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">para generar objetos ts (time series)</span>
</pre>
</div>
<p>
y además fijamos los parámetros por defecto para las figuras en <code>png</code>
del notebook
</p>
<div class="org-src-container">
<pre class="src src-jupyter-R"><span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">fijamos el tama&#241;o de las figuras que se generan en el notebook</span>
<span style="color: #D0372D;">options</span>(repr.plot.width = 12, repr.plot.height = 4, repr.plot.res = 200)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc14eef1" class="outline-2">
<h2 id="orgc14eef1"><span class="section-number-2">1.</span> Series temporales vs datos de sección cruzada</h2>
<div class="outline-text-2" id="text-1">
<p>
Corresponden a observaciones de un mismo objeto a lo largo del
tiempo. El índice indica el instante de cada medición. <i>El orden
cronológico puede ser crucial</i> al modelar los datos.
</p>

<ul class="org-ul">
<li>El motivo es que frecuentemente el valor medido en un instante de
tiempo está relacionado con otras mediciones próximas en el tiempo
(<i>correlación serial</i>).</li>

<li>Si es así, ya no deberíamos asumir que las variables aleatorias del
proceso estocástico subyacente, \(\boldsymbol{X}=(X_t\mid
  t\in\mathbb{Z})\), son independientes entre sí.</li>
</ul>

<p>
Esto tiene importantes implicaciones en las técnicas de análisis y
los modelos a utilizar.
</p>
<p>
Veamos algunos ejemplos de series temporales&#x2026;
</p>
</div>
<ol class="org-ol">
<li><a id="org8249d46"></a>Población en Australia<br />
<div class="outline-text-5" id="text-1-0-0-1">
<div class="org-src-container">
<pre class="src src-jupyter-R">PoblacionAustralia_ts = as.ts( read.zoo(<span style="color: #008000;">'datos/PoblacionAustralia.csv'</span>, 
                                        header=<span style="color: #6434A3;">TRUE</span>,
                                        index.column = 1, 
                                        sep=<span style="color: #008000;">","</span>, 
                                        FUN = as.yearmon))
p <span style="color: #D0372D;">&lt;-</span> autoplot(PoblacionAustralia_ts)
p <span style="color: #D0372D;">&lt;-</span> p + labs(y = <span style="color: #008000;">"Habitantes"</span>, x = <span style="color: #008000;">"A&#241;os"</span>) + ggtitle(<span style="color: #008000;">"Poblaci&#243;n australiana (datos anuales)"</span>)
p <span style="color: #D0372D;">&lt;-</span> p + scale_x_continuous(breaks = scales::pretty_breaks(n = 20))
p 
</pre>
</div>


<div id="orgd23b9fe" class="figure">
<p><img src="./img/lecc05/PoblacionAustralia.png" alt="PoblacionAustralia.png" width="900px" />
</p>
</div>
</div>
</li>
<li><a id="org3d221d8"></a>PIB UEM<br />
<div class="outline-text-5" id="text-1-0-0-2">
<div class="org-src-container">
<pre class="src src-jupyter-R">PIB_UEM_df <span style="color: #D0372D;">&lt;-</span> read_csv(<span style="color: #008000;">"datos/PIB_UEM.csv"</span>,
                     show_col_types = <span style="color: #6434A3;">FALSE</span>)
fmt <span style="color: #D0372D;">&lt;-</span> <span style="color: #008000;">"%YQ%q"</span>
PIB_UEM_df$Time <span style="color: #D0372D;">&lt;-</span> as.yearqtr(PIB_UEM_df$obs, format = fmt)
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">head(PIB_UEM_df,3)</span>
P <span style="color: #D0372D;">&lt;-</span> ggplot(PIB_UEM_df, aes(Time, PIB))
P <span style="color: #D0372D;">&lt;-</span> P + geom_point() + geom_line()
P <span style="color: #D0372D;">&lt;-</span> P + scale_x_continuous(breaks = scales::pretty_breaks(n = 15))
P <span style="color: #D0372D;">&lt;-</span> P + labs(y = <span style="color: #008000;">"Miles de millones de euros"</span>, x = <span style="color: #008000;">"A&#241;os"</span>)
P <span style="color: #D0372D;">&lt;-</span> P + ggtitle(<span style="color: #008000;">"PIB UEM a precios corrientes (datos trimestrales). Fuente Banco de Espa&#241;a"</span>)
P
</pre>
</div>


<div id="orgb9bb156" class="figure">
<p><img src="./img/lecc05/PIB_UEM.png" alt="PIB_UEM.png" width="900px" />
</p>
</div>
</div>
</li>
<li><a id="org0aaea87"></a>Temperatura media en el Parque del Retiro. Madrid<br />
<div class="outline-text-5" id="text-1-0-0-3">
<div class="org-src-container">
<pre class="src src-jupyter-R">TemperaturaRetiro_df <span style="color: #D0372D;">&lt;-</span> read_csv(<span style="color: #008000;">"datos/Retiro.txt"</span>, show_col_types = <span style="color: #6434A3;">FALSE</span>)
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">A&#241;adimos fechas</span>
TemperaturaRetiro_df$Time <span style="color: #D0372D;">&lt;-</span> as.yearmon(1985 + seq(0, nrow(TemperaturaRetiro_df)-1)/12)

P <span style="color: #D0372D;">&lt;-</span> ggplot(TemperaturaRetiro_df, aes(Time, TemperaturaMedia))
P <span style="color: #D0372D;">&lt;-</span> P + geom_line() <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">+ geom_point() </span>
P <span style="color: #D0372D;">&lt;-</span> P + scale_x_continuous(breaks = scales::pretty_breaks(n = 25))
P <span style="color: #D0372D;">&lt;-</span> P + labs(y = <span style="color: #008000;">"Grados Celsius"</span>, x = <span style="color: #008000;">"A&#241;os"</span>) 
P <span style="color: #D0372D;">&lt;-</span> P + ggtitle(<span style="color: #008000;">"Temperatura media mensual en el Parque del Retiro. Fuente: Comunidad de Madrid"</span>)
P
</pre>
</div>


<div id="org4164a0c" class="figure">
<p><img src="./img/lecc05/TemperaturaReriro.png" alt="TemperaturaReriro.png" width="900px" />
</p>
</div>
</div>
</li>
<li><a id="org56413c1"></a>Rendimiento porcentual diario del IBEX 35 (std)<br />
<div class="outline-text-5" id="text-1-0-0-4">
<div class="org-src-container">
<pre class="src src-jupyter-R">IBEX35_ts = as.ts( read.csv.zoo(<span style="color: #008000;">"datos/IBEX35.csv"</span>, 
                                strip.white = <span style="color: #6434A3;">TRUE</span>))
P <span style="color: #D0372D;">&lt;-</span> autoplot(IBEX35_ts) + scale_y_continuous(breaks = scales::pretty_breaks(n = 12))
p <span style="color: #D0372D;">&lt;-</span> P + labs(y = <span style="color: #008000;">"Desviaciones t&#237;picas"</span>, x = <span style="color: #008000;">"D&#237;as"</span>)
p <span style="color: #D0372D;">&lt;-</span> P + ggtitle(<span style="color: #008000;">"Rendimiento porcentual diario del IBEX 35 (std.). Fuente: Archivo Prof. Miguel Jerez"</span>)
p 
</pre>
</div>


<div id="orge76232b" class="figure">
<p><img src="./img/lecc05/IBEX35.png" alt="IBEX35.png" width="900px" />
</p>
</div>

<ul class="org-ul">
<li>Datos centrados y estandarizados, i.e. el eje vertical está en desviaciones típicas.</li>
<li>Los <i>volatility clustering</i> son característicos de series financieras de alta frecuencia.</li>
</ul>
</div>
</li>
<li><a id="orgba87de9"></a>Producción de cemento<br />
<div class="outline-text-5" id="text-1-0-0-5">
<div class="org-src-container">
<pre class="src src-jupyter-R">ProduccionCemento_df <span style="color: #D0372D;">&lt;-</span> read_csv(<span style="color: #008000;">"datos/ProduccionCemento.csv"</span>,
                     show_col_types = <span style="color: #6434A3;">FALSE</span>)
fmt <span style="color: #D0372D;">&lt;-</span> <span style="color: #008000;">"%YM%m"</span>
ProduccionCemento_df$Time <span style="color: #D0372D;">&lt;-</span> as.yearmon(ProduccionCemento_df$obs, format = fmt)
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">head(ProduccionCemento_df,3)</span>
P <span style="color: #D0372D;">&lt;-</span> ggplot(ProduccionCemento_df, aes(Time, ProduccionCemento))
P <span style="color: #D0372D;">&lt;-</span> P + geom_line() <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">+ geom_point() </span>
P <span style="color: #D0372D;">&lt;-</span> P + scale_x_continuous(breaks = scales::pretty_breaks(n = 25))
P <span style="color: #D0372D;">&lt;-</span> P + labs(y = <span style="color: #008000;">"Miles de Toneladas m&#233;tricas"</span>, x = <span style="color: #008000;">"A&#241;os"</span>)
P <span style="color: #D0372D;">&lt;-</span> P + ggtitle(<span style="color: #008000;">"Producci&#243;n de cemento (Datos mensuales). Fuente Banco de Espa&#241;a"</span>)
P
</pre>
</div>


<div id="orgb514438" class="figure">
<p><img src="./img/lecc05/ProduccionCemento.png" alt="ProduccionCemento.png" width="900px" />
</p>
</div>
</div>
</li>
</ol>
<div id="outline-container-org8bb2aeb" class="outline-3">
<h3 id="org8bb2aeb"><span class="section-number-3">1.1.</span> Correlación serial vs muestreo aleatorio simple</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Con datos de
</p>
<dl class="org-dl">
<dt>sección cruzada</dt><dd>solemos asumir que el muestreo es aleatorio
simple
<ul class="org-ul">
<li>i.e., los datos son realizaciones de variables aleatorias i.i.d.</li>
</ul></dd>

<dt>series temporales</dt><dd><p>
dicha asunción resulta generalmente errónea
</p>
<ul class="org-ul">
<li>con frecuencia el nivel esperado (o la volatilidad) parece cambiar con \(t\)</li>
<li>con frecuencia hay dependencia temporal (correlación serial).</li>
</ul>

<p>
<b>Ejemplo</b>: no parece aceptable asumir que \(ProdCemento_{1960M01}\) se
distribuye igual que \(ProdCemento_{2000M04}\) (ni que sea
independiente de \(ProdCemento_{1959M01}\)).
</p></dd>
</dl>
<p>
Veamos por qué esto genera dificultades&#x2026;
</p>
<p>
Consideremos el proceso estocástico \[\boldsymbol{X}=(X_t \mid
t=0,\pm1,\pm2,\ldots).\] Caracterizar su distribución conjunta (todos
los momentos) es demasiado ambicioso.
</p>

<p>
Así que, tentativamente, vamos a fijarnos <i>solo</i> en los dos primeros
momentos:
</p>

<p>
\[E(X_t)={\color{blue}{ \mu_t}}\quad\text{ y }\quad
Cov(X_t,X_k)=E\big[(X_t-\mu_t)(X_k-\mu_k)\big]={\color{blue}{\gamma_{t,k}}};\quad t,k\in\mathbb{Z}\]
</p>

<p>
(si \(\;k=t\;\) entonces \(\;\gamma_{t,t}=Var(X_t)=\sigma^2_t\)).
</p>
<p>
Si el proceso \(\boldsymbol{X}\) fuera gaussiano, conocer estos
<i>parámetros</i> bastaría para caracterizar la distribución conjunta. Pero
aún así&#x2026;
</p>

<ul class="org-ul">
<li>necesitaríamos para cada \(X_t\) una muestra suficiente para estimar los parámetros 
<ul class="org-ul">
<li>pero en una serie temporal tenemos una sola realización de cada \(X_t\).</li>
</ul></li>

<li>Además&#x2026; para cada variable aleatoria \(X_t\) hay infinitos parámetros.</li>
</ul>
</div>
</div>
<div id="outline-container-org3daa50f" class="outline-3">
<h3 id="org3daa50f"><span class="section-number-3">1.2.</span> Simplificación del escenario</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Si \(\boldsymbol{X}\) es <a href="./Lecc01.slides.html#/3/1">débilmente estacionario</a> se reduce drásticamente el número de parámetros:
</p>
\begin{eqnarray}
  E(X_t)  & = \mu \\
  Cov(X_t,X_{t-k}) & = \gamma_k
\end{eqnarray}

<p>
El desafío para el analista es (y nótese el abuso de lenguaje)
</p>
<dl class="org-dl">
<dt>primero</dt><dd>transformar los datos para lograr que sean "<b><i>estacionarios</i></b>".
<ul class="org-ul">
<li>(Algo vimos en la lección 1))</li>
</ul></dd>
<dt>después</dt><dd>transformar los datos estacionarios en "<b><i>ruido blanco</i></b>"
<ul class="org-ul">
<li>(Es lo que iniciaremos en esta lección y las siguientes)</li>
</ul></dd>
</dl>

<p>
Todo este proceso constituye la especificación y ajuste de un modelo a
la serie temporal.
</p>
<p>
Antes de atacar los temas de especificación y ajuste de modelos,
debemos estudiar un poco los procesos estocásticos débilmente
estacionarios que vamos a utilizar.
</p>
</div>
</div>
</div>
<div id="outline-container-org635beae" class="outline-2">
<h2 id="org635beae"><span class="section-number-2">2.</span> Procesos estocásticos de segundo orden</h2>
<div class="outline-text-2" id="text-2">
<p>
El ambiente natural para estudiar las propiedades de segundo orden de
una colección de variables aleatorias es el espacio de variables
aleatorias \(X\) definidas en un espacio de probabilidad tales que
\[E(X)=0 \quad\text{y}\quad E(X^2)<\infty\] donde \(E\) es el operador
esperanza. Denotaremos este espacio con \(H\).
</p>
</div>
<div id="outline-container-org78b5d6c" class="outline-3">
<h3 id="org78b5d6c"><span class="section-number-3">2.1.</span> Un poco de geometría</h3>
<div class="outline-text-3" id="text-2-1">
<p>
El espacio, dotado de producto escalar y norma \[\langle X \mid Y
\rangle=E(XY),\qquad \lVert X \rVert= \sqrt{E(X^2)},\qquad X,Y \in
H,\] es un espacio de Hilbert,
</p>

<p>
Nótese que como las variables de \(H\) tienen esperanza cero, el
producto escalar entre \(X,Y\in H\) también es \[\langle X \mid Y
\rangle=Cov(X,Y).\] Por tanto, en este espacio \(H\) la noción
geométrica de ortogonalidad coincide con la noción estadística de <i>no
correlación</i>. Por tanto, en este contexto los términos producto
escalar, covarianza y esperanza del producto serán intercambiables.
</p>
<p>
Una colección de variables aleatorias pertenecientes a \(H\)
\[\boldsymbol{X}=(X_t\mid t\in\mathbb{Z}) \;\text{ con }\; X_t\in H\]
se denomina <i>proceso estocástico de segundo orden</i>.
</p>

<p>
Si \(\boldsymbol{Y}=(Y_t\mid t\in\mathbb{Z})\) es tal que
\(E(Y_t)=\mu\ne0\), entonces \(\boldsymbol{Y}\) no es de segundo orden.
</p>

<p>
Pero basta restar \(\mu\) de cada \(Y_t\) para tener un proceso
\((\boldsymbol{Y}-\mu\boldsymbol{1})\) de segundo orden.
</p>

<p>
<i>Por ello siempre asumiremos</i> (sin pérdida de generalidad) <i>que las
variables aleatorias de los procesos estocásticos de esta lección</i> (y
la siguiente) <i>tienen esperanza cero</i>.
</p>
</div>
</div>
<div id="outline-container-orgb0f529e" class="outline-3">
<h3 id="orgb0f529e"><span class="section-number-3">2.2.</span> Primeros momentos de procesos estocásticos de segundo orden</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Si \(E(X_t)<\infty\) para \(t\in\mathbb{Z}\), entonces \(E(\boldsymbol{X})\) es
la secuencia \[E(\boldsymbol{X})=\big(E(X_t)\mid
t\in\mathbb{Z}\big)=\sum\nolimits_{t\in\mathbb{Z}} E(X_t)
z^t=\big(\ldots,\;E(X_{-1}),\;E(X_{0}),\;E(X_{1}),\ldots\big)\]
</p>

<p>
Si \(\boldsymbol{X}\) tiene segundos momentos finitos, la secuencia de
autocovarianzas <span class="underline">de orden \(k\)</span> es
</p>
\begin{align*}
%Cov(\boldsymbol{X},\boldsymbol{X}*z^k) = &
%E\Big(\big[\boldsymbol{X}-E(\boldsymbol{X})\big]\odot\big[(\boldsymbol{X}-E(\boldsymbol{X}))*z^k\big]\Big)\\
\left.\Big(Cov(X_t,X_{t-k})\right| t\in\mathbb{Z}\Big)
= & 
%\left.\Big(E\big[\big(X_t-E(X_t)\big)\big(X_{t-k}-E(X_{t-k})\big)\big]\; \right| t\in\mathbb{Z}\Big)\\
%=&
% \sum_{t\in\mathbb{Z}} \gamma_{_{k,t}} z^t
(\gamma_{_{k,t}}\mid t\in\mathbb{Z})\\ % \;=\;
= &
(\ldots,\,\gamma_{_{k,-1}},\,{\color{blue}{\gamma_{_{k,0}}}},\,\gamma_{_{k,1}},\,\gamma_{_{k,2}},\ldots);\quad k\in\mathbb{Z}.
\end{align*}
<p>
(nótese que la secuencia solo contiene covarianzas de orden \(k\)) 
</p>
<p>
Así, para cada par \((k,t)\), tenemos la covarianza \(\gamma_{k,t}\) entre
\(X_t\) y \(X_{t-k}\). Por tanto, en general, tenemos una esperanza para
cada \(t\) y una covarianza de orden \(k\) para cada \(t\). Dado que \(t\)
recorre todos los números enteros, ¡esto son muchos momentos!  <span class="underline">Por
eso necesitamos reducir el número de parámetros restringiéndonos a
procesos estocásticos débilmente estacionarios</span>.
</p>
</div>
</div>
<div id="outline-container-org81a45c2" class="outline-3">
<h3 id="org81a45c2"><span class="section-number-3">2.3.</span> Procesos estocásticos (débilmente) estacionarios y la ACF</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Un proceso estocástico de segundo orden \(\boldsymbol{X}\) se dice que
es <i>débilmente estacionario</i> si \(E(X_t)=\mu\) para todo
\(t\in\mathbb{Z}\) y la covarianza entre \(X_s\) y \(X_t\) solo depende de
la diferencia \(s-t\) para todo \(s,t\in\mathbb{Z}\).
</p>

<p>
En tal caso, definimos la <span class="underline">función de autocovarianzas</span> como:
\[\boldsymbol{\gamma} = (\gamma_{k}\mid k\in\mathbb{Z}) =
(\ldots,\,\gamma_{-1},\,{\color{blue}{\gamma_{0}}},\,\gamma_{1},\,\gamma_{2},\ldots)
\;=\;\sum_{-\infty}^{\infty} \gamma_k z^k.\]
</p>
<p>
<b>Propiedades</b> de la función de autocovarianzas \(\boldsymbol{\gamma}\) (ACF):
</p>
<ul class="org-ul">
<li>\(\gamma_0\geq0\)</li>
<li>\(\boldsymbol{\gamma}\) <span class="underline">es definida positiva</span>; y por tanto,
<ul class="org-ul">
<li>\(\boldsymbol{\gamma}\) es simétrica: \(\gamma_k=\gamma_{-k}\)</li>
<li>\(\boldsymbol{\gamma}\) es acotada: \(|\gamma_k|\leq\gamma_0\)</li>
</ul></li>
</ul>

<p>
Y llamamos <i>función de autocorrelación</i> (ACF) a la
secuencia:
\(\;\boldsymbol{\rho}=\frac{1}{\gamma_0}(\boldsymbol{\gamma})
=\sum\limits_{k\in\mathbb{Z}}\frac{\gamma_k}{\gamma_0}z^k\).
</p>
</div>
</div>
</div>
<div id="outline-container-org3c75a39" class="outline-2">
<h2 id="org3c75a39"><span class="section-number-2">3.</span> Notación: convolución y el operador retardo</h2>
<div class="outline-text-2" id="text-3">
<p>
Sea \(\boldsymbol{a}\) una secuencia de números y sea \(\boldsymbol{X}\) un
proceso estocástico tales que <span class="underline">la suma</span>
\[\sum\limits_{k=-\infty}^{\infty}a_kX_{t-k}\;\] <span class="underline">converge</span> para todo
\(t.\;\) Entonces:
</p>
<p>
Definimos el producto convolución (\(*\)) de \(\boldsymbol{a}\) con
\(\boldsymbol{X}\) como el proceso estocástico:
\[\boldsymbol{a}*\boldsymbol{X}=\left(\left.\sum_{r+s=t} a_r X_s
\right| t\in\mathbb{Z}\right)\] es decir
\[(\boldsymbol{a}*\boldsymbol{X})_t=\sum_{r+s=t} a_r X_s,\quad
\text{para } t\in\mathbb{Z}.\] Por tanto, cada elemento de
\((\boldsymbol{a}*\boldsymbol{X})\) es una combinación de variables
aleatorias de \(\boldsymbol{X}\)
</p>
<p>
Podemos aplicar el operador \(\mathsf{B}\) sobre los elementos de un proceso estocástico \(\boldsymbol{X}\).
\[\mathsf{B} X_t = X_{t−1},\quad \text{para } t\in\mathbb{Z}.\]
</p>

<p>
Aplicando el operador \(\mathsf{B}\) repetidamente tenemos \[\mathsf{B}^k X_t =
X_{t−k},\quad \text{para } t,z\in\mathbb{Z}\] 
</p>
<p>
Así, para el polinomio \(\boldsymbol{a}(z)=a_0+a_1z+a_2z^2+a_3z^3\), y el proceso estocástico \(\boldsymbol{Y}\)
</p>
\begin{align*}
\boldsymbol{a}(\mathsf{B})Y_t 
& = (a_0+a_1\mathsf{B}+a_2\mathsf{B}^2+a_3\mathsf{B}^3) Y_t \\
% & = a_0 Y_t + a_1 \mathsf{B}^1 Y_t + a_2 \mathsf{B}^2 Y_t + a_3 \mathsf{B}^3 Y_t \\
& = a_0Y_t+a_1Y_{t-1}+a_2Y_{t-2}+a_3Y_{t-3} \\
% & =\sum\nolimits_{r=0}^3 a_r Y_{t-r} \\
& =(\boldsymbol{a}*\boldsymbol{Y})_t,\quad \text{para } t\in\mathbb{Z}
\end{align*}
<p>
Y en general, si la suma \(\sum\limits_{k=-\infty}^{\infty}a_kY_{t-k}\)
converge para todo \(t\), entonces
</p>

\begin{align*}
\boldsymbol{a}(\mathsf{B})Y_t 
& = (\cdots+a_{-2}\mathsf{B}^{-2}+a_{-1}\mathsf{B}^{-1}+a_0+a_1\mathsf{B}+a_2\mathsf{B}^2+\cdots) Y_t \\
% & = a_0 Y_t + a_1 \mathsf{B}^1 Y_t + a_2 \mathsf{B}^2 Y_t + a_3 \mathsf{B}^3 Y_t \\
& = \cdots+a_{-2}Y_{t+2}+a_{-1}Y_{t+1}+a_0Y_t+a_1Y_{t-1}+a_2Y_{t-2}+\cdots \\
% & =\sum\nolimits_{r=0}^3 a_r Y_{t-r} \\
& =(\boldsymbol{a}*\boldsymbol{y})_t,\quad \text{para } t\in\mathbb{Z}
\end{align*}
</div>
</div>
<div id="outline-container-org0cc7da8" class="outline-2">
<h2 id="org0cc7da8"><span class="section-number-2">4.</span> Ejemplos de procesos (débilmente) estacionarios</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgb0bb725" class="outline-3">
<h3 id="orgb0bb725"><span class="section-number-3">4.1.</span> Proceso de ruido blanco</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Una secuencia \(\boldsymbol{U}=(U_t\mid t\in\mathbb{Z})\) de variables
aleatorias incorreladas y tales que \[E(U_t)=0\quad\text{ y }\quad
Var(U_t)=E(U_t^2)=\sigma^2\] para \(\;t\in\mathbb{Z}\;\) y
\(\;0<\sigma^2<\infty\;\) se llama <i>proceso de ruido blanco</i>.
\(\quad\boldsymbol{U}\sim WN(0,\sigma^2)\).
</p>
<p>
Al ser variables aleatorias incorreladas, su función de
autocovarianzas es \[\boldsymbol{\gamma}(z)\;=\;\sigma^2
z^0\;=\;(\ldots,0,0,\sigma^2,0,0,\ldots)\]
</p>

<ul class="org-ul">
<li>Es el proceso estacionario (no trivial) más sencillo.</li>
<li>Este proceso es el pilar sobre el que definiremos el resto de
ejemplos.</li>
</ul>
</div>
</div>
<div id="outline-container-org71db0d9" class="outline-3">
<h3 id="org71db0d9"><span class="section-number-3">4.2.</span> Procesos lineales</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Sea \(\boldsymbol{U}\sim WN(0,\sigma^2)\) y sea \(\boldsymbol{b}\in
\ell^2\); una secuencia de <span class="underline">cuadrado sumable</span>
\(\;\sum\limits_{j\in\mathbb{Z}}{b}_j^2<\infty\).
</p>
<p>
Denominamos <i>proceso lineal</i> al proceso estocástico
\(\boldsymbol{X}=\boldsymbol{b}*\boldsymbol{U}\) cuyos elementos son \[X_t
\;=\;(\boldsymbol{b}*\boldsymbol{U})_t
\;=\;\boldsymbol{b}(B)U_t \;=\;\sum_{j=-\infty}^\infty {b}_j
U_{t-j};\qquad t\in\mathbb{Z}.\]
</p>

<p>
\(\boldsymbol{b}(B)\) se denomina <i>función de transferencia</i> del
filtro lineal que relaciona \(X_t\) con \(U_t\).
</p>
<p>
El proceso lineal es <i>``causal''</i> si además \(\boldsymbol{b}\) es
una <span class="underline">serie formal</span> (i.e.,
\(cogrado(\boldsymbol{b})\geq{\color{blue}{0}}\))
\[X_t=\sum_{j=0}^\infty {b}_j U_{t-j};\qquad
t\in\mathbb{Z}\] \(\;\) (pues cada \(X_t\) es una suma de variables "<i>del
presente y/o el pasado</i>").
</p>
<p>
La clase de procesos lineales incluye muchas e importantes subclases
de procesos, algunas de las cuales son objeto principal de estudio de
este curso.
</p>
</div>
<div id="outline-container-org74f4fb6" class="outline-4">
<h4 id="org74f4fb6"><span class="section-number-4">4.2.1.</span> Media móvil infinita. MA(\(\infty\))</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Sea \(\;\boldsymbol{U}\sim WN(0,\sigma^2)\;\) y sea
\(\;\boldsymbol{\psi}\in \ell^2\;\) una serie formal con <span class="underline">infinitos
términos NO nulos</span>; entonces el proceso estocástico
\(\boldsymbol{\psi}*\boldsymbol{U}\), cuyos elementos son \[X_t
\;=\;(\boldsymbol{\psi}*\boldsymbol{U})_t
\;=\;\boldsymbol{\psi}(B)U_t \;=\;\sum_{j=0}^\infty \psi_j
U_{t-j};\qquad t\in\mathbb{Z}\] se denomina proceso de <i>media móvil
infinita</i> MA(\(\infty\)).
</p>
<p>
Algunas clases de procesos lineales tienen una representación
parsimoniosa, pues basta un número finito de parámetros para
representarlos completamente. Por ejemplo, cuando
\(\boldsymbol{\psi}\) tiene un número finito de términos no nulos&#x2026;
</p>
</div>
</div>
<div id="outline-container-org7f39499" class="outline-4">
<h4 id="org7f39499"><span class="section-number-4">4.2.2.</span> Proceso de media móvil de orden \(q\). MA(\(q\))</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
Sea \(\;\boldsymbol{U}\sim WN(0,\sigma^2)\;\) y sea
\(\;\boldsymbol{\theta}\;\) un <span class="underline">polinomio de grado \(q\)</span> con
\({\color{#008000}{\theta_{0}=1}}\); entonces el proceso estocástico
\(\boldsymbol{\theta}*\boldsymbol{U}\), cuyos elementos son \[X_t
\;=\;(\boldsymbol{\theta}*\boldsymbol{U})_t
\;=\;\boldsymbol{\theta}(B)U_t \;=\;\sum_{j=0}^q\theta_j
U_{t-j};\qquad t\in\mathbb{Z}\] se denomina proceso de <i>media móvil</i>
MA(\(q\)).
</p>

<p>
Es decir, si \(\;\boldsymbol{\theta}=(1-\theta_1z-\cdots-\theta_qz^q)\;\):
\[ X_t = U_t - \theta_1 U_{t-1} - \cdots - \theta_q U_{t-q}.\]
</p>


<p>
Hay otros procesos lineales con representación parsimoniosa.
</p>
</div>
</div>
<div id="outline-container-orgef8ae33" class="outline-4">
<h4 id="orgef8ae33"><span class="section-number-4">4.2.3.</span> Proceso autorregresivo de orden \(p\). AR(\(p\))</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
Sea \(\;\boldsymbol{U}\sim WN(0,\sigma^2)\;\), se denomina <i>proceso
autorregresivo de orden \(p\)</i> a aquel proceso estocástico estacionario
\(\;\boldsymbol{X}\;\) que es la solución de la siguiente ecuación en
diferencias \[\boldsymbol{\phi}*\boldsymbol{X}=\boldsymbol{U}\] donde
\(\;\boldsymbol{\phi}\;\) un <span class="underline">polinomio de grado \(p\)</span> con \({\color{#008000}{\phi_{0}=1}}\);
</p>
<p>
Por tanto, \[(\boldsymbol{\phi}*\boldsymbol{X})_t=
\boldsymbol{\phi}(\mathsf{B})X_t= \sum_{j=0}^p \phi_j X_{t-j} = U_t.\]
</p>

<p>
Si \(\;\boldsymbol{\phi}=(1-\phi_1z-\cdots-\phi_pz^p)\;\) entonces
\(\boldsymbol{X}=(X_t\mid t\in\mathbb{Z})\) es solución de la ecuación:
\[X_t + \phi_1 X_{t-1} + \cdots + \phi_q X_{t-q} = U_t.\]
</p>
<p>
El problema con la anterior definición es que la ecuación
\(\boldsymbol{\phi}*\boldsymbol{X}=\boldsymbol{U}\) no tiene solución
única (y en algunos casos ninguna solución es
estacionaria). Despejemos \(\boldsymbol{X}\) para verlo.
</p>

<p>
Multiplicando ambos lados de la ecuación por una inversa de
\(\boldsymbol{\phi}\) tenemos
\[\boldsymbol{X}=inversa(\boldsymbol{\phi})*\boldsymbol{U}.\] Y si
denotamos la secuencia \(inversa(\boldsymbol{\phi})\) con
\(\boldsymbol{a}\) entonces
\[X_t=\boldsymbol{a}(\mathsf{B})U_t=\sum_{j\in\mathbb{Z}} a_j
U_{t-j}.\]
</p>
<p>
Pero&#x2026; ¿Qué secuencia \(\boldsymbol{a}\) usamos como inversa de
\(\boldsymbol{\phi}\)? Recuerde que hay infinitas y la mayoría no son
sumables (si el polinomio \(\boldsymbol{\phi}\) tiene raíces unitarias
ninguna lo es).
</p>

<div class="org-center">
<p>
En tal caso la expresión
\(\;\boldsymbol{a}(\mathsf{B})U_t=\sum\limits_{j=-\infty}^\infty a_j
U_{t-j}\;\) carece de sentido (pues no converge).
</p>
</div>
<p>
<b>Requisitos</b> sobre el polinomio autorregresivo \(\boldsymbol{\phi}.\;\) Para que el proceso AR exista y sea:
</p>

<ol class="org-ol">
<li><p>
<span class="underline">estacionario</span>, exigiremos que \(\boldsymbol{\phi}\)
<span class="underline">no tenga raíces de módulo 1</span>.
</p>

<p>
Entonces existe una única inversa absolutamente sumable: \(\boldsymbol{\phi}^{-1} \in
   \ell^1\subset\ell^2\).
</p>

<p>
La inversa \(\boldsymbol{a}=\boldsymbol{\phi}^{-1}\) corresponde a la
única solución <i>estacionaria</i> de
\(\boldsymbol{\phi}*\boldsymbol{X}=\boldsymbol{U}\).  (Si
\(\boldsymbol{\phi}\) tuviera raíces de módulo 1 no existiría ni
\(\boldsymbol{\phi}^{-1}\), ni la solución estacionaria).
</p>

<p>
\[X_t=\boldsymbol{\phi}^{-1}(\mathsf{B})U_t=\sum_{j=-\infty}^\infty a_j U_{t-j}\]
</p></li>

<li><p>
<span class="underline">lineal causal</span> exigiremos que las raíces de \(\boldsymbol{\phi}\)
sean mayores que 1 en valor absoluto (<span class="underline">raíces fuera del círculo
unidad</span>):
\(\boldsymbol{\phi}^{-1}=\boldsymbol{\phi}^{-\triangleright}\).
</p>

<p>
\[X_t=\boldsymbol{\phi}^{-1}(\mathsf{B})U_t=\sum_{j=0}^\infty a_j U_{t-j}\]
</p></li>
</ol>
<p>
El siguiente modelo lineal es una combinación (o generalización) de
los dos anteriores.
</p>
</div>
</div>
<div id="outline-container-org9f6caaf" class="outline-4">
<h4 id="org9f6caaf"><span class="section-number-4">4.2.4.</span> Proceso autorregresivo de media móvil. ARMA(\(p,q\))</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
Sea \(\;\boldsymbol{U}\sim WN(0,\sigma^2)\;\), se denomina <i>proceso
autorregresivo de media móvil \((p,q)\)</i> al proceso estocástico
estacionario \(\;\boldsymbol{X}\;\) que es la solución de la ecuación en
diferencias:
\[\boldsymbol{\phi}*\boldsymbol{X}=\boldsymbol{\theta}*\boldsymbol{U}\]
donde el polinomio <i>autorregresivo</i> \(\;\boldsymbol{\phi}\;\) tiene
<span class="underline">grado \(p\)</span> con \({\color{#008000}{\phi_{0}=1}}\) y con todas sus raíces
fuera del círculo unidad (<i>por los motivos anteriormente vistos</i>); y
el polinomio <i>de media móvil</i> \(\;\boldsymbol{\theta}\;\) es <span class="underline">de grado
\(q\)</span> con \({\color{#008000}{\theta_{0}=1}}\); 
</p>

<p>
\[\text{es decir,}\qquad
\boldsymbol{X}=\frac{\boldsymbol{\theta}}{\boldsymbol{\phi}}*\boldsymbol{U};
\qquad\text{donde}\;
\frac{\boldsymbol{\theta}}{\boldsymbol{\phi}}\equiv\boldsymbol{\phi}^{-1}*\boldsymbol{\theta}\]
</p>

<p>
Tanto \(\boldsymbol{\phi}^{-1}\) como \(\boldsymbol{\theta}\) son
absolutamente sumables y como \(\ell^1\) es un anillo,
\(\boldsymbol{\phi}^{-1}*\boldsymbol{\theta}\equiv\frac{\boldsymbol{\theta}}{\boldsymbol{\phi}}\in\ell^1\)
(también es absolutamente sumable y por tanto de cuadrado sumable),
consecuentemente el proceso estocástico es un proceso lineal.
\[X_t=\frac{\boldsymbol{\theta}}{\boldsymbol{\phi}}(\mathsf{B})U_t=\sum_{j=0}^\infty
a_j U_{t-j}\] donde
\(\boldsymbol{a}=\boldsymbol{\phi}^{-1}*\boldsymbol{\theta}\).
</p>
</div>
</div>
<div id="outline-container-orgd5e31dd" class="outline-4">
<h4 id="orgd5e31dd"><span class="section-number-4">4.2.5.</span> Proceso autorregresivo de media móvil con media no nula</h4>
<div class="outline-text-4" id="text-4-2-5">
<p>
Consideremos un proceso \(\boldsymbol{Y}\) con media
distinta de cero, es decir, \[E(Y_t)=\mu\ne0\] y definamos la
secuencia constante \(\boldsymbol{\mu}=\sum\limits_{j\in\mathbb{Z}} \mu
z^j=(\ldots,\mu,\mu,\mu,\ldots)\). 
\medskip
</p>

<p>
Decimos que \(\boldsymbol{Y}\) es un proceso ARMA(\(p,q\)) con media
distinta de cero si \(\boldsymbol{X}\) es ARMA(\(p,q\))
\[\boldsymbol{\phi}*\boldsymbol{X}=\boldsymbol{\theta}*\boldsymbol{U}\]
donde \(\boldsymbol{X}=\boldsymbol{Y}-\boldsymbol{\mu}\) es
evidentemente un proceso de media cero.  Por tanto
</p>
\begin{align*}
\boldsymbol{\phi}*(\boldsymbol{Y}-\boldsymbol{\mu})=&\boldsymbol{\theta}*\boldsymbol{U}\\
\boldsymbol{\phi}*\boldsymbol{Y}-\boldsymbol{\phi}*\boldsymbol{\mu}=&\boldsymbol{\theta}*\boldsymbol{U}\\
\boldsymbol{\phi}*\boldsymbol{Y}=&\boldsymbol{\phi}*\boldsymbol{\mu}+ \boldsymbol{\theta}*\boldsymbol{U}\\
\end{align*}
<p>
Es decir, si \(\boldsymbol{\phi}(\mathsf{B})\) es
\(\;1-\phi_1\mathsf{B}-\phi_2\mathsf{B}^2-\cdots-\phi_p\mathsf{B}^p,\;\)
entonces \[\boldsymbol{\phi}(B){Y_t}=c+\boldsymbol{\theta}(B){U_t}\]
donde \[\;c=(1-\phi_1-\phi_2-\cdots-\phi_p)\mu\;\] y donde
\(\;\mu=E(Y_t)\), es un proceso autorregresivo de media móvil
ARMA(\(p,q\)) <i>con media no nula</i>.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org48d78cf" class="outline-2">
<h2 id="org48d78cf"><span class="section-number-2">5.</span> Primeros momentos de procesos lineales causales</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org8d85be3" class="outline-3">
<h3 id="org8d85be3"><span class="section-number-3">5.1.</span> Esperanza y autocovarianzas de un proceso lineal causal</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Sea \(\;\boldsymbol{X}=\boldsymbol{\psi}*\boldsymbol{U},\;\) donde
\(\boldsymbol{\psi}\) es una serie formal de cuadrado sumable
y donde \(\;\boldsymbol{U}\sim
WN(0,\sigma^2).\quad\) Recordando que la convolución es una operación
lineal: \[E(\boldsymbol{X}) =E(\boldsymbol{\psi}*\boldsymbol{U})
=\boldsymbol{\psi}*E(\boldsymbol{U})
=\boldsymbol{\psi}*\boldsymbol{0}=\boldsymbol{0}.\] Consecuentemente,
la covarianza de orden \(k\) para cada \(X_t\) es
</p>
\begin{align*}
\gamma_{_{k,t}} = & E\Big[\big(\boldsymbol{\psi}(\mathsf{B})X_t\big)\cdot \big(\boldsymbol{\psi}(\mathsf{B}) X_{t-k}\big)\Big] 
\\ = &
E\Big[
 (\psi_0U_{t}+\psi_1U_{t-1}+\psi_2U_{t-2}\cdots)
 (\psi_0U_{t-k}+\psi_1U_{t-k-1}+\psi_2U_{t-k-2}\cdots)\Big]
\\ = &
\sigma^2\sum\nolimits_{j\in\mathbb{Z}}\psi_{j+k}\psi_j
\qquad \text{ ya que }\; E(U_hU_j)=0\; \text{ si } \;j\ne h,
\end{align*}
<p>
que no depende de \(t\) (\(\boldsymbol{X}\) es estacionario). Es más, por
la última ecuación de la lección 4 \[\;\gamma_k \;=\;
\sigma^2\sum\nolimits_{j\in\mathbb{Z}}\psi_{j+k}\psi_j \;=\;
\sigma^2\big(\boldsymbol{\psi}(z)*\boldsymbol{\psi}(z^{-1})\big)_k
\qquad \text{ para } k\in\mathbb{Z}.\] Y, por tanto
</p>
\begin{equation}
 \label{eqAutoCovarianzaProcesoLineal}
 \boldsymbol{\gamma}=\sigma^2\boldsymbol{\psi}(z)*\boldsymbol{\psi}(z^{-1})
\end{equation}
<p>
con grado igual al grado de \(\boldsymbol{\psi}\) y cogrado igual a
menos el grado de \(\boldsymbol{\psi}\).
</p>
</div>
</div>
<div id="outline-container-orgab0395e" class="outline-3">
<h3 id="orgab0395e"><span class="section-number-3">5.2.</span> Covarianza cruzada entre dos procesos lineales causales</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Sean \(\;\boldsymbol{W}=\boldsymbol{\theta}*\boldsymbol{U}\quad\) e
\(\quad\boldsymbol{Y}=\boldsymbol{\psi}*\boldsymbol{U},\quad\) donde
\(\boldsymbol{\theta}\) y \(\boldsymbol{\psi}\) son series formales de
cuadrado sumable y donde \(\;\boldsymbol{U}\sim
WN(0,\sigma^2)\).
</p>

<p>
Repitiendo los mismos pasos que en el caso de la autocovarianza,
llegamos a que la <span class="underline">función de covarianzas cruzadas</span> es la secuencia
</p>
\begin{equation}
 \label{eqCovarianzaCruzadaProcesosLineales}
 \boldsymbol{\gamma_{_{\boldsymbol{W},\boldsymbol{Y}}}} =
 \sigma^2 \boldsymbol{\theta}(z)*\boldsymbol{\psi}(z^{-1})
\end{equation}
<p>
con grado igual al grado de \(\boldsymbol{\theta}\) y cogrado igual a menos
el grado de \(\boldsymbol{\psi}\).
</p>
</div>
</div>
<div id="outline-container-org392fab2" class="outline-3">
<h3 id="org392fab2"><span class="section-number-3">5.3.</span> Las Ecuaciones de Yule-Walker para un AR(\(p\)) estacionario</h3>
<div class="outline-text-3" id="text-5-3">
<p>
<i>Por una parte</i> (lado izquierdo):
</p>

<p>
Si \(\boldsymbol{X}\) es un proceso (débilmente) estacionario con
\(E(\boldsymbol{X})=\boldsymbol{0}\;\) y \(\;\boldsymbol{\phi}\) es una serie
formal absolutamente sumable; entonces para \(t,k\in\mathbb{Z}\)
</p>
\begin{equation}
  E\Big[\Big(\boldsymbol{\phi}(\mathsf{B})X_t\Big)\cdot X_{t-k}\Big]
  \quad = \quad
  \boldsymbol{\phi}(\mathsf{B})E\big(X_t\cdot X_{t-k}\big)
  \quad = \quad
  \boldsymbol{\phi}(\mathsf{B})\gamma_k
  \label{eqnLadoIzquierdoYW}
\end{equation}
<p>
que no depende de \(t\), por ser \(\boldsymbol{X}\) es un proceso
(débilmente) estacionario.
</p>
<p>
<i>Por otra parte</i> (lado derecho):
</p>

<p>
Si \(\boldsymbol{X}\) tiene representación
\(\;\boldsymbol{X}=\boldsymbol{\psi}*\boldsymbol{U}\) donde
\(\;\boldsymbol{U}\sim WN(0,\sigma^2)\) y \(\boldsymbol{\psi}\in\ell^2\) es una
serie formal con \(\psi_0=1\); es decir, si es un proceso lineal causal
</p>

<p>
\[\quad X_t=U_t + \sum\nolimits_{j=1}^\infty \psi_j U_{t-j},\]
entonces para \(t,k\in\mathbb{Z}\)
</p>
\begin{equation}
  E[U_t\cdot X_{t-k}] = E\Big[U_t\Big(U_{t-k} + \sum\nolimits_{j=1}^\infty \psi_j U_{t-k-j}\Big) \Big]=
  \begin{cases}
  \sigma^2 & \text{cuando } k=0\\
  0 & \text{cuando } k\ne0
  \end{cases}
  \label{eqnLadoDerechoYW}
\end{equation}
<p>
Sea un AR(\(p\)) estacionario:
\(\;\;\boldsymbol{\phi}(\mathsf{B})X_t=U_t\;\;\) donde
\(\;\;\boldsymbol{\phi}(z)=1-\phi_1z^1-\cdots-\phi_pz^p.\;\)
Multiplicando por \(X_{t-k}\) y tomando esperanzas:
\[E\Big[\Big(\boldsymbol{\phi}(\mathsf{B})X_t\Big)\cdot X_{t-k}\Big] =
E[U_t\cdot X_{t-k}]\]
</p>

<p>
<b>para \(k=0\):</b> \(\quad\) (por \(\ref{eqnLadoIzquierdoYW}\) y \(\ref{eqnLadoDerechoYW}\))
\[\fbox{\(\boldsymbol{\phi}(\mathsf{B})\gamma_0=\sigma^2\)}
\quad\Rightarrow\quad
\gamma_0-\phi_1\gamma_1-\cdots-\phi_p\gamma_p=\sigma^2
\quad\Rightarrow\quad \sigma^2=\gamma_0-\sum\nolimits_{j=1}^p\phi_j\gamma_j.\]
Dividiendo por \(\gamma_0\) (y recordando que \(\rho_0=1\)):
\[\boldsymbol{\phi}(\mathsf{B})\rho_0=\frac{\sigma^2}{\gamma_0}
\quad\Rightarrow\quad
\fbox{\(\gamma_0=\frac{\sigma^2}{\boldsymbol{\phi}(\mathsf{B})\rho_0}\)}
\quad\Rightarrow\quad
\gamma_0=\frac{\sigma^2}{1-\sum\nolimits_{j=1}^p\phi_j\rho_j}.\] 
</p>

<p>
<b>para \(k>0\):</b> \(\quad\) (por \(\ref{eqnLadoIzquierdoYW}\) y \(\ref{eqnLadoDerechoYW}\))
\[\fbox{\(\boldsymbol{\phi}(\mathsf{B})\gamma_k=0\)}
\quad\Rightarrow\quad
\gamma_k-\phi_1\gamma_{k-1}-\cdots-\phi_p\gamma_{k-p}=0
\quad\Rightarrow\quad \gamma_k=\sum\nolimits_{j=1}^p\phi_j\gamma_{k-j}.\] 
Dividiendo por \(\gamma_0\):
\[\fbox{\(\boldsymbol{\phi}(\mathsf{B})\rho_k=0\)}
\quad\Rightarrow\quad
\rho_k-\phi_1\rho_{k-1}-\cdots-\phi_p\rho_{k-p}=0
\quad\Rightarrow\quad \rho_k=\sum\nolimits_{j=1}^p\phi_j\rho_{k-j}.\]
</p>

<p>
Por tanto, la estructura autorregresiva del proceso impone que las
autocovarianzas (y las autocorrelaciones) verifiquen las ecuaciones de
Yule-Walker.
</p>
</div>
</div>
<div id="outline-container-org749e848" class="outline-3">
<h3 id="org749e848"><span class="section-number-3">5.4.</span> Función de autocovarianzas para un ARMA(\(p,q\))</h3>
<div class="outline-text-3" id="text-5-4">
<p>
Sea un ARMA(\(p,q\)) estacionario:
\(\boldsymbol{\phi}(\mathsf{B}){X_t}=\boldsymbol{\theta}(\mathsf{B}){U_t}\;\)
donde \(\boldsymbol{\phi}\) y \(\boldsymbol{\theta}\) no tienen raíces
comunes. Multiplicando por \(X_{t-k}\), tomando esperanzas y
sustituyendo \(X_{t-k}\) por su representación MA(\(\infty\)), donde
\(\boldsymbol{\psi}=\frac{\boldsymbol{\theta}}{\boldsymbol{\phi}}\):
\[\underbrace{E\Big[\Big(\boldsymbol{\phi}(\mathsf{B})X_t\Big)\cdot
X_{t-k}\Big]}_{\boldsymbol{\phi}(\mathsf{B})\gamma_k\;(\text{por
\ref{eqnLadoIzquierdoYW}})} =
E\Big[\Big(\boldsymbol{\theta}(\mathsf{B})U_t\Big)\cdot X_{t-k}\Big]
\;=\;
\underbrace{E\Big[\Big(\boldsymbol{\theta}(\mathsf{B})U_t\Big)\cdot
\Big(\boldsymbol{\psi}(\mathsf{B})U_{t-k}\Big)\Big]}_{\boldsymbol{\gamma_{_{\boldsymbol{W},\boldsymbol{Y}}}}(k)}\]
Donde hemos usando \(\eqref{eqnLadoIzquierdoYW}\) y renombrando
\(\;\boldsymbol{\theta}(\mathsf{B})U_t=\boldsymbol{W}\;\) y
\(\;\boldsymbol{\psi}(\mathsf{B})U_t=\boldsymbol{Y}.\;\) Así:
</p>
\begin{align*}
  \boldsymbol{\phi}(\mathsf{B})\gamma_k & = \boldsymbol{\gamma_{_{\boldsymbol{W},\boldsymbol{Y}}}}(k)\\
  & =  \sigma^2 \Big(\boldsymbol{\theta}(z)*\boldsymbol{\psi}(z^{-1})\Big)_k & \text{por } \eqref{eqCovarianzaCruzadaProcesosLineales}
\end{align*}
<p>
Y como \(\boldsymbol{\theta}(z)*\boldsymbol{\psi}(z^{-1})\) tiene grado \(q\) y cogrado \(-\infty\)
</p>
\begin{equation}
  \boldsymbol{\phi}(\mathsf{B})\gamma_k = 
  \begin{cases}
     0 & k > q\quad \text{(como en un AR)}\\
    \sigma^2 \Big(\boldsymbol{\theta}(z)*\boldsymbol{\psi}(z^{-1})\Big)_k & k\leq q
   \quad \text{(que depende de $\boldsymbol{\theta}$ y $\boldsymbol{\phi}$)}
  \end{cases}
\end{equation}
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Marcos Bujosa</p>
<p class="date">Created: 2024-09-27 vie 17:38</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
