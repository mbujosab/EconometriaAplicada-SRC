{"cells":[{"cell_type":"markdown","id":"baaca2b1-b248-4d79-8274-637bf634ed2e","metadata":{},"source":"Econometría Aplicada. Lección 5\n===============================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"9b8ee7c9-f616-4281-9e60-d28c9b4f21fb","metadata":{},"source":["<div class=\"abstract\" id=\"orgdd98501\">\n<p>\nEsta lección veremos las dificultades que ocasiona la correlación\nserial y algunos tipos de procesos débilmente estacionarios que nos\npermitirán lidiar con ella. En particular veremos los procesos\nlineales, su valor esperado y su función de autocovarianzas, la\nfunción de covarianzas cruzadas entre dos procesos lineales, y las\necuaciones de Yule-Walker.\n</p>\n\n</div>\n\n-   [lección en html](https://mbujosab.github.io/EconometriaAplicada-SRC/Lecc05.html)\n-   [lección en mybinder](https://mybinder.org/v2/gh/mbujosab/EconometriaAplicada-SRC/HEAD?labpath=Lecc05.ipynb)\n\n"]},{"cell_type":"markdown","id":"19959fed-868e-47c8-acbe-a303882a60c0","metadata":{"slideshow":{"slide_type":"notes"}},"source":["#### Carga de algunas librerías de R\n\n"]},{"cell_type":"markdown","id":"b38653b9-c395-40f3-9dfc-f78bd579e489","metadata":{"slideshow":{"slide_type":"notes"}},"source":["Primero cargamos la librería `tfarima` (Repositorio Cran:\n[https://cran.r-project.org/web/packages/tfarima/index.html](https://cran.r-project.org/web/packages/tfarima/index.html);\nrepositorio GitHub: [https://github.com/gallegoj/tfarima](https://github.com/gallegoj/tfarima))\n\n"]},{"cell_type":"code","execution_count":1,"id":"a85b5042-1c4a-413c-88ca-a63131a27aba","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["library(tfarima)      # librería de José Luis Gallego para Time Series\nlibrary(readr)        # para leer ficheros CSV\nlibrary(ggplot2)      # para el scatterplot (alternaticamente library(tidyverse))\nlibrary(ggfortify)    # para pintar series temporales\nlibrary(jtools)       # para representación resultados estimación\nlibrary(zoo)          # para generar objetos ts (time series)"]},{"cell_type":"markdown","id":"1b2b3a4d-fb56-41f7-9af8-dda75931a724","metadata":{"slideshow":{"slide_type":"notes"}},"source":["y además fijamos los parámetros por defecto para las figuras en `png`\ndel notebook\n\n"]},{"cell_type":"code","execution_count":1,"id":"65556226-7059-4504-a728-9c1405c10575","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["# fijamos el tamaño de las figuras que se generan en el notebook\noptions(repr.plot.width = 12, repr.plot.height = 4, repr.plot.res = 200)"]},{"cell_type":"markdown","id":"a7de301a-5c7f-415c-b4e9-d983560fcfc3","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Series temporales vs datos de sección cruzada\n\n"]},{"cell_type":"markdown","id":"e896aa26-5ba9-4dd3-8035-34569da3b790","metadata":{},"source":["Corresponden a observaciones de un mismo objeto a lo largo del\ntiempo. El índice indica el instante de cada medición. *El orden\ncronológico puede ser crucial* al modelar los datos.\n\n-   El motivo es que frecuentemente el valor medido en un instante de\n    tiempo está relacionado con otras mediciones próximas en el tiempo\n    (*correlación serial*).\n\n-   Si es así, ya no deberíamos asumir que las variables aleatorias del\n    proceso estocástico subyacente, $\\boldsymbol{X}=(X_t\\mid\n      t\\in\\mathbb{Z})$, son independientes entre sí.\n\nEsto tiene importantes implicaciones en las técnicas de análisis y\nlos modelos a utilizar.\n\nVeamos algunos ejemplos de series temporales&#x2026;\n\n"]},{"cell_type":"markdown","id":"546c46e8-7f9e-4479-bb1b-3229d4c1b88e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["##### Población en Australia\n\n"]},{"cell_type":"code","execution_count":1,"id":"34a7b917-80b0-413c-a508-746983d7cbc5","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["PoblacionAustralia_ts = as.ts( read.zoo('datos/PoblacionAustralia.csv', \n                                        header=TRUE,\n                                        index.column = 1, \n                                        sep=\",\", \n                                        FUN = as.yearmon))\np <- autoplot(PoblacionAustralia_ts)\np <- p + labs(y = \"Habitantes\", x = \"Años\") + ggtitle(\"Población australiana (datos anuales)\")\np <- p + scale_x_continuous(breaks = scales::pretty_breaks(n = 20))\np"]},{"cell_type":"markdown","id":"b66c38f2-409f-4bbc-88ab-d7836318768c","metadata":{},"source":["![img](./img/lecc05/PoblacionAustralia.png)\n\n"]},{"cell_type":"markdown","id":"de68da44-d0ff-48b2-a3be-38dd1d12596e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["##### PIB UEM\n\n"]},{"cell_type":"code","execution_count":1,"id":"a355d89c-3b51-4be0-8d3d-f49b6a28d601","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["PIB_UEM_df <- read_csv(\"datos/PIB_UEM.csv\",\n                     show_col_types = FALSE)\nfmt <- \"%YQ%q\"\nPIB_UEM_df$Time <- as.yearqtr(PIB_UEM_df$obs, format = fmt)\n# head(PIB_UEM_df,3)\nP <- ggplot(PIB_UEM_df, aes(Time, PIB))\nP <- P + geom_point() + geom_line()\nP <- P + scale_x_continuous(breaks = scales::pretty_breaks(n = 15))\nP <- P + labs(y = \"Miles de millones de euros\", x = \"Años\")\nP <- P + ggtitle(\"PIB UEM a precios corrientes (datos trimestrales). Fuente Banco de España\")\nP"]},{"cell_type":"markdown","id":"8020eef7-6f12-453d-a08f-5d0cd2aee82b","metadata":{},"source":["![img](./img/lecc05/PIB_UEM.png)\n\n"]},{"cell_type":"markdown","id":"f2dd497e-1755-4680-bfdd-a408ce4a5b30","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["##### Temperatura media en el Parque del Retiro. Madrid\n\n"]},{"cell_type":"code","execution_count":1,"id":"4665252a-3a14-41b3-ae44-736e1b879271","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["TemperaturaRetiro_df <- read_csv(\"datos/Retiro.txt\", show_col_types = FALSE)\n# Añadimos fechas\nTemperaturaRetiro_df$Time <- as.yearmon(1985 + seq(0, nrow(TemperaturaRetiro_df)-1)/12)\n\nP <- ggplot(TemperaturaRetiro_df, aes(Time, TemperaturaMedia))\nP <- P + geom_line() # + geom_point() \nP <- P + scale_x_continuous(breaks = scales::pretty_breaks(n = 25))\nP <- P + labs(y = \"Grados Celsius\", x = \"Años\") \nP <- P + ggtitle(\"Temperatura media mensual en el Parque del Retiro. Fuente: Comunidad de Madrid\")\nP"]},{"cell_type":"markdown","id":"de01d1b8-f068-4591-8bab-8a5188ed97ee","metadata":{},"source":["![img](./img/lecc05/TemperaturaReriro.png)\n\n"]},{"cell_type":"markdown","id":"3a0cc36e-d93d-44a9-b076-95e32e9a5111","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["##### Rendimiento porcentual diario del IBEX 35 (std)\n\n"]},{"cell_type":"code","execution_count":1,"id":"e25d009a-98c8-4c1e-9055-1a2969e11a61","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["IBEX35_ts = as.ts( read.csv.zoo(\"datos/IBEX35.csv\", \n                                strip.white = TRUE))\nP <- autoplot(IBEX35_ts) + scale_y_continuous(breaks = scales::pretty_breaks(n = 12))\np <- P + labs(y = \"Desviaciones típicas\", x = \"Días\")\np <- P + ggtitle(\"Rendimiento porcentual diario del IBEX 35 (std.). Fuente: Archivo Prof. Miguel Jerez\")\np"]},{"cell_type":"markdown","id":"f7e52c40-2caf-4149-b639-186d16b4b87a","metadata":{},"source":["![img](./img/lecc05/IBEX35.png)\n\n-   Datos centrados y estandarizados, i.e. el eje vertical está en desviaciones típicas.\n-   Los *volatility clustering* son característicos de series financieras de alta frecuencia.\n\n"]},{"cell_type":"markdown","id":"e7ab0445-2565-40cd-965a-3a794e5b7d70","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["##### Producción de cemento\n\n"]},{"cell_type":"code","execution_count":1,"id":"140c59f9-d250-4e92-8cfe-f268747fb381","metadata":{"slideshow":{"slide_type":"notes"}},"outputs":[],"source":["ProduccionCemento_df <- read_csv(\"datos/ProduccionCemento.csv\",\n                     show_col_types = FALSE)\nfmt <- \"%YM%m\"\nProduccionCemento_df$Time <- as.yearmon(ProduccionCemento_df$obs, format = fmt)\n# head(ProduccionCemento_df,3)\nP <- ggplot(ProduccionCemento_df, aes(Time, ProduccionCemento))\nP <- P + geom_line() # + geom_point() \nP <- P + scale_x_continuous(breaks = scales::pretty_breaks(n = 25))\nP <- P + labs(y = \"Miles de Toneladas métricas\", x = \"Años\")\nP <- P + ggtitle(\"Producción de cemento (Datos mensuales). Fuente Banco de España\")\nP"]},{"cell_type":"markdown","id":"d7b24e37-fbaf-4e74-93ed-a1ec982b3e66","metadata":{},"source":["![img](./img/lecc05/ProduccionCemento.png)\n\n"]},{"cell_type":"markdown","id":"adb447ca-9128-439b-ac34-472634fd702c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Correlación serial vs muestreo aleatorio simple\n\n"]},{"cell_type":"markdown","id":"a70dbe50-1521-4fa9-bf68-198cd5b21bd2","metadata":{},"source":["Con datos de\n\n-   **sección cruzada:** solemos asumir que el muestreo es aleatorio\n    simple\n    -   i.e., los datos son realizaciones de variables aleatorias i.i.d.\n\n-   **series temporales:** dicha asunción resulta generalmente errónea\n    \n    -   con frecuencia el nivel esperado (o la volatilidad) parece cambiar con $t$\n    -   con frecuencia hay dependencia temporal (correlación serial).\n    \n    **Ejemplo**: no parece aceptable asumir que $ProdCemento_{1960M01}$ se\n    distribuye igual que $ProdCemento_{2000M04}$ (ni que sea\n    independiente de $ProdCemento_{1959M01}$).\n\nVeamos por qué esto genera dificultades&#x2026;\n\n"]},{"cell_type":"markdown","id":"7322ab4a-6260-47f3-8354-82f62ad860ce","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Consideremos el proceso estocástico $$\\boldsymbol{X}=(X_t \\mid\nt=0,\\pm1,\\pm2,\\ldots).$$ Caracterizar su distribución conjunta (todos\nlos momentos) es demasiado ambicioso.\n\n"]},{"cell_type":"markdown","id":"7542d651-c3c3-40ef-9f11-18f5ddd1c4cd","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Así que, tentativamente, vamos a fijarnos *solo* en los dos primeros\nmomentos:\n\n$$E(X_t)={\\color{blue}{ \\mu_t}}\\quad\\text{ y }\\quad\nCov(X_t,X_k)=E\\big[(X_t-\\mu_t)(X_k-\\mu_k)\\big]={\\color{blue}{\\lambda_{t,k}}};\\quad t,k\\in\\mathbb{Z}$$\n\n(si $\\;k=t\\;$ entonces $\\;\\lambda_{t,t}=Var(X_t)=\\sigma^2_t$).\n\nSi el proceso $\\boldsymbol{X}$ fuera gaussiano, conocer estos\n*parámetros* bastaría para caracterizar la distribución conjunta. Pero\naún así&#x2026;\n\n"]},{"cell_type":"markdown","id":"10b60310-5460-4a1d-b3d2-cb572932edfa","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["-   necesitaríamos para cada $X_t$ una muestra suficiente para estimar los parámetros \n    -   pero en una serie temporal tenemos una sola realización de cada $X_t$.\n\n-   Además&#x2026; para cada variable aleatoria $X_t$ hay infinitos parámetros.\n\n"]},{"cell_type":"markdown","id":"68702bc4-235f-4339-86e6-4e4edca2b8b6","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Tipos de procesos estocásticos que simplifican el escenario\n\n"]},{"cell_type":"markdown","id":"92d9d05f-8d62-4a3e-911f-0c1eb0bc0b16","metadata":{},"source":["-   Si es [débilmente estacionario](./Lecc01.slides.html#/3/1) se reduce drásticamente el número de\n    parámetros:\n    \n    \\begin{eqnarray}\n    E(X_t)  & = \\mu \\\\\n    Cov(X_t,X_{t-k}) & = \\gamma_k\n    \\end{eqnarray}\n-   Si además es i.i.d. podemos interpretar la serie temporal como una\n    realización de un muestreo aleatorio simple.\n\n"]},{"cell_type":"markdown","id":"4b0c583d-9396-4106-8544-3721df3f8cf2","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El desafío para el analista es (y nótese el abuso de lenguaje)\n\n-   **primero:** transformar los datos para lograr que sean \"***estacionarios***\".\n    -   (Algo vimos en la lección 1))\n-   **después:** transformar los datos estacionarios en una secuencia de\n    \"**datos *i.i.d***\" \n    -   (Aún no lo hemos visto)\n\nTodo este proceso constituye la especificación y ajuste de un modelo a\nla serie temporal.\n\n"]},{"cell_type":"markdown","id":"e6d130a0-f690-4837-ac11-418dd3a085a9","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Antes de atacar los temas de especificación y ajuste de modelos,\ndebemos estudiar un poco los procesos estocásticos débilmente\nestacionarios que vamos a utilizar.\n\n"]},{"cell_type":"markdown","id":"16cb1c17-4593-4587-9a97-18e8c26b14f4","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Procesos estocásticos de segundo orden\n\n"]},{"cell_type":"markdown","id":"d05543d8-4278-4c1c-a000-7f7603bd06a8","metadata":{},"source":["El ambiente natural para estudiar las propiedades de segundo orden de\nuna colección de variables aleatorias es el espacio de variables\naleatorias $X$ definidas en un espacio de probabilidad tales que\n$$E(X)=0 \\quad\\text{y}\\quad E(X^2)<\\infty$$ donde $E$ es el operador\nesperanza. Denotaremos este espacio con $H$.\n\n"]},{"cell_type":"markdown","id":"c2774916-7174-4aea-8b01-bb62f9a0773b","metadata":{"slideshow":{"slide_type":"notes"}},"source":["### Un poco de geometría\n\n"]},{"cell_type":"markdown","id":"bf1e398a-2bf9-43f2-bfd0-eaf2d7699d25","metadata":{"slideshow":{"slide_type":"notes"}},"source":["El espacio, dotado de producto escalar y norma $$\\langle X \\mid Y\n\\rangle=E(XY),\\qquad \\lVert X \\rVert= \\sqrt{E(X^2)},\\qquad X,Y \\in\nH,$$ es un espacio de Hilbert,\n\nNótese que como las variables de $H$ tienen esperanza cero, el\nproducto escalar entre $X,Y\\in H$ también es $$\\langle X \\mid Y\n\\rangle=Cov(X,Y).$$ Por tanto, en este espacio $H$ la noción\ngeométrica de ortogonalidad coincide con la noción estadística de *no\ncorrelación*. Por tanto, en este contexto los términos producto\nescalar, covarianza y esperanza del producto serán intercambiables.\n\n"]},{"cell_type":"markdown","id":"d3e5a1b4-cc6d-4cab-8b72-3c9a74f3b375","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Una colección de variables aleatorias pertenecientes a $H$\n$$\\boldsymbol{X}=(X_t\\mid t\\in\\mathbb{Z}) \\;\\text{ con }\\; X_t\\in H$$\nse denomina *proceso estocástico de segundo orden*.\n\n"]},{"cell_type":"markdown","id":"c2666a7f-d66c-4271-a452-6ebc9e5362a5","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Si $\\boldsymbol{Y}=(Y_t\\mid t\\in\\mathbb{Z})$ es tal que\n$E(Y_t)=\\mu\\ne0$, entonces $\\boldsymbol{Y}$ no es de segundo orden.\n\nPero basta restar $\\mu$ de cada $Y_t$ para tener un proceso\n$(\\boldsymbol{Y}-\\boldsymbol{\\mu})$ de segundo orden.\n\n*Por ello siempre asumiremos* (sin pérdida de generalidad) *que las\nvariables aleatorias de los procesos estocásticos de esta lección* (y\nla siguiente) *tienen esperanza cero*.\n\n"]},{"cell_type":"markdown","id":"cf1a053f-12bc-4aef-9705-138320869679","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Función de covarianzas\n\n"]},{"cell_type":"markdown","id":"b777c719-9cef-4eda-bf05-cfc18a94c6d8","metadata":{},"source":["La *función de covarianzas* de un proceso estocástico $\\boldsymbol{X}$\nsegundo orden es $$\\boldsymbol{\\gamma}=(\\gamma_{s,t}\\mid\ns,t\\in\\mathbb{Z})$$ donde $\\gamma_{s,t}=Cov(X_s,X_t)\\quad\ns,t\\in\\mathbb{Z}$.\n\nAsí, para cada par $(s,t)$, la covarianza $\\gamma_{s,t}$ mide la\ndependencia lineal entre $X_s$ y $X_t$.\n\n"]},{"cell_type":"markdown","id":"ade61d7f-b30d-43cc-b06a-4d7399ad46a0","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Esta función $\\boldsymbol{\\gamma}$ es demasiado general, por eso nos\nrestringiremos a la subclase de procesos estocásticos *débilmente\nestacionarios*; pues simplifican enormemente la *función de\ncovarianzas* $\\boldsymbol{\\gamma}$.\n\n"]},{"cell_type":"markdown","id":"f621a499-4200-4718-a16e-2406c269710d","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Proceso estocástico (débilmente) estacionario y su ACF\n\n"]},{"cell_type":"markdown","id":"6f92fd8f-1b5d-4c71-b857-cf4d59cdd00e","metadata":{},"source":["Un proceso estocástico de segundo orden $\\boldsymbol{X}$ se dice que\nes *débilmente estacionario* si la covarianza entre $X_s$ y $X_t$\nsolo depende de la diferencia $s-t$ para todo $s,t\\in\\mathbb{Z}$.\n\nEn tal caso se denomina *función de <u>auto</u>-covarianzas* de\n$\\boldsymbol{X}$ a la secuencia\n$$\\boldsymbol{\\gamma}(z)\\;=\\;(\\gamma_k\\mid\nk\\in\\mathbb{Z})\\;=\\;\\sum_{-\\infty}^{\\infty} \\gamma_k z^k$$ donde\n$\\;\\gamma_k=Cov(X_{t+k}, X_{t})\\;$ para $k\\in\\mathbb{Z}$.\n\n**Propiedades** de la función de autocovarianzas $\\boldsymbol{\\gamma}$ (ACF):\n\n-   $\\gamma_0\\geq0$\n-   $\\boldsymbol{\\gamma}$ <u>es definida positiva</u>; y por tanto,\n    -   $\\boldsymbol{\\gamma}$ es simétrica: $\\gamma_k=\\gamma_{-k}$\n    -   $\\boldsymbol{\\gamma}$ es acotada: $|\\gamma_k|\\leq\\gamma_0$\n\nllamamos *función de autocorrelación* (ACF) a la\nsecuencia:\n$\\;\\boldsymbol{\\rho}=\\frac{1}{\\gamma_0}(\\boldsymbol{\\gamma})\n=\\sum\\limits_{k\\in\\mathbb{Z}}\\frac{\\gamma_k}{\\gamma_0}z^k$.\n\n"]},{"cell_type":"markdown","id":"8003e6b7-f161-4dc9-a7b6-5baae63dcfda","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Procesos estocásticos y notación\n\n"]},{"cell_type":"markdown","id":"1366f006-e44b-4ed8-82e5-041f98315ba3","metadata":{},"source":["Los procesos estocásticos se pueden sumar y se pueden multiplicar por\nescalares.\n\nSi $\\boldsymbol{X}$ e $\\boldsymbol{Y}$ son dos procesos estocásticos y\n$\\;a\\in\\mathbb{R}$, entonces $$\\boldsymbol{X}+\\boldsymbol{Y}=(X_t+Y_t\n\\mid t\\in\\mathbb{Z})\\qquad\\text{y}\\qquad a\\boldsymbol{X}=\\big(a(X_t)\n\\mid t\\in\\mathbb{Z}\\big).$$ El conjunto de procesos estocásticos junto\ncon la suma elemento a elemento y el producto por escalares\nconstituyen un espacio vectorial.\n\n"]},{"cell_type":"markdown","id":"12be7795-285a-4df9-a826-a3c9e76094b7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Consideremos el proceso estocástico \n$$\\boldsymbol{X}=(X_t \\mid t=0,\\pm1,\\pm2,\\ldots).$$\n\nLo podemos denotar con una función generatriz (como hicimos con las\nsecuencias) $$\\boldsymbol{X} \\quad = \\quad \\sum_{t=-\\infty}^\\infty X_t\nz^t \\quad\\equiv\\quad \\boldsymbol{X}(z)$$ Recuerde que esto no es una\nsuma; es una secuencia de variables aleatorias\n$$\\sum_{t=-\\infty}^\\infty X_t z^t = (\\ldots,\\ X_{-2},\\ X_{-1},\\\nX_{0},\\ X_{1},\\ X_{2},\\ldots)$$\n\n"]},{"cell_type":"markdown","id":"99c5e6cc-832c-400c-a648-db5806edf3ef","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $\\boldsymbol{a}$ una secuencia de números y $\\boldsymbol{X}$ un\nproceso estocástico tales que <u>la suma</u>\n$\\sum\\limits_{k=-\\infty}^{\\infty}a_kX_{t-k}\\;$ <u>converge</u> para todo\n$t.\\;$ Entonces:\n\nDefinimos el producto convolución ($∗$) de $\\boldsymbol{a}$ con $\\boldsymbol{X}$ como el proceso estocástico:\n$$\\boldsymbol{a}*\\boldsymbol{X}=\\left(\\left.\\sum_{r+s=t} a_r X_s \\right| t\\in\\mathbb{Z}\\right)$$\nes decir\n$$(\\boldsymbol{a}*\\boldsymbol{X})_t=\\sum_{r+s=t} a_r X_s,\\quad \\text{para } t\\in\\mathbb{Z}.$$\nPor tanto, cada elemento de $(\\boldsymbol{a}*\\boldsymbol{X})$ es una combinación de variables aleatorias de $\\boldsymbol{X}$\n\n"]},{"cell_type":"markdown","id":"3eb5000c-6656-43ab-9f26-b6b878ffdc15","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Podemos aplicar el operador $\\mathsf{B}$ sobre los elementos de un proceso estocástico $\\boldsymbol{X}$.\n$$\\mathsf{B} X_t = X_{t−1},\\quad \\text{para } t\\in\\mathbb{Z}.$$\n\nAplicando el operador $\\mathsf{B}$ repetidamente tenemos $$\\mathsf{B}^k X_t =\nX_{t−k},\\quad \\text{para } t,z\\in\\mathbb{Z}$$\n\n"]},{"cell_type":"markdown","id":"ad51a7b8-8fa7-45ae-84e3-c9ec075cd1ff","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Así, para el polinomio $\\boldsymbol{a}(z)=a_0+a_1z+a_2z^2+a_3z^3$, y el proceso estocástico $\\boldsymbol{Y}$\n\n\\begin{align*}\n\\boldsymbol{a}(\\mathsf{B})Y_t \n& = (a_0+a_1\\mathsf{B}+a_2\\mathsf{B}^2+a_3\\mathsf{B}^3) Y_t \\\\\n% & = a_0 Y_t + a_1 \\mathsf{B}^1 Y_t + a_2 \\mathsf{B}^2 Y_t + a_3 \\mathsf{B}^3 Y_t \\\\\n& = a_0Y_t+a_1Y_{t-1}+a_2Y_{t-2}+a_3Y_{t-3} \\\\\n% & =\\sum\\nolimits_{r=0}^3 a_r Y_{t-r} \\\\\n& =(\\boldsymbol{a}*\\boldsymbol{Y})_t,\\quad \\text{para } t\\in\\mathbb{Z}\n\\end{align*}\n\n"]},{"cell_type":"markdown","id":"eee0c6fa-f0d2-418d-9d20-66336ada3afd","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Y en general, si la suma $\\sum\\limits_{k=-\\infty}^{\\infty}a_kY_{t-k}$\nconverge para todo $t$, entonces\n\n\\begin{align*}\n\\boldsymbol{a}(\\mathsf{B})Y_t \n& = (\\cdots+a_{-2}\\mathsf{B}^{-2}+a_{-1}\\mathsf{B}^{-1}+a_0+a_1\\mathsf{B}+a_2\\mathsf{B}^2+\\cdots) Y_t \\\\\n% & = a_0 Y_t + a_1 \\mathsf{B}^1 Y_t + a_2 \\mathsf{B}^2 Y_t + a_3 \\mathsf{B}^3 Y_t \\\\\n& = \\cdots+a_{-2}Y_{t+2}+a_{-1}Y_{t+1}+a_0Y_t+a_1Y_{t-1}+a_2Y_{t-2}+\\cdots \\\\\n% & =\\sum\\nolimits_{r=0}^3 a_r Y_{t-r} \\\\\n& =(\\boldsymbol{a}*\\boldsymbol{y})_t,\\quad \\text{para } t\\in\\mathbb{Z}\n\\end{align*}\n\n"]},{"cell_type":"markdown","id":"1b18e979-e3ee-4b08-a461-f100b453f472","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Ejemplos de procesos (débilmente) estacionarios\n\n"]},{"cell_type":"markdown","id":"10740e98-eee8-4fa5-99bc-71aae807d137","metadata":{},"source":["### Proceso de ruido blanco\n\n"]},{"cell_type":"markdown","id":"10cb7dc6-ceef-4df3-b1a7-2576334b7545","metadata":{},"source":["Una secuencia $\\boldsymbol{U}=(U_t\\mid t\\in\\mathbb{Z})$ de variables\naleatorias incorreladas y tales que $$E(U_t)=0\\quad\\text{ y }\\quad\nVar(U_t)=E(U_t^2)=\\sigma^2$$ para $\\;t\\in\\mathbb{Z}\\;$ y\n$\\;0<\\sigma^2<\\infty\\;$ se llama *proceso de ruido blanco*.\n$\\quad\\boldsymbol{U}\\sim WN(0,\\sigma^2)$.\n\n"]},{"cell_type":"markdown","id":"8ee1b450-a773-459d-96a3-b3212fa53cbc","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Al ser variables aleatorias incorreladas, su función de\nautocovarianzas es $$\\boldsymbol{\\gamma}(z)\\;=\\;\\sigma^2\nz^0\\;=\\;(\\ldots,0,0,\\sigma^2,0,0,\\ldots)$$\n\n-   Es el proceso estacionario (no trivial) más sencillo.\n-   Este proceso es el pilar sobre el que definiremos el resto de\n    ejemplos.\n\n"]},{"cell_type":"markdown","id":"b97f0f21-192f-4de4-a4ae-03133809eb50","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Procesos lineales\n\n"]},{"cell_type":"markdown","id":"35ffe80d-ec7e-4ded-8c4d-3a226a2e088e","metadata":{},"source":["Sea $\\boldsymbol{U}\\sim WN(0,\\sigma^2)$ y sea $\\boldsymbol{b}\\in\n\\ell^2$; una secuencia de <u>cuadrado sumable</u>\n$\\;\\sum\\limits_{j\\in\\mathbb{Z}}{b}_j^2<\\infty$.\n\nDenominamos *proceso lineal* al proceso estocástico\n$\\boldsymbol{X}=\\boldsymbol{b}*\\boldsymbol{U}$ cuyos elementos son $$X_t\n\\;=\\;(\\boldsymbol{b}*\\boldsymbol{U})_t\n\\;=\\;\\boldsymbol{b}(B)U_t \\;=\\;\\sum_{j=-\\infty}^\\infty {b}_j\nU_{t-j};\\qquad t\\in\\mathbb{Z}.$$\n\n"]},{"cell_type":"markdown","id":"b75d8344-6e83-407a-a72e-eb08737bd4d8","metadata":{"slideshow":{"slide_type":"notes"}},"source":["$\\boldsymbol{b}(B)$ se denomina *función de transferencia* del\nfiltro lineal que relaciona $X_t$ con $U_t$.\n\n"]},{"cell_type":"markdown","id":"25e04810-00de-41ee-88b6-3354b532b3cf","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El proceso lineal es *\\`\\`causal''* si además $\\boldsymbol{b}$ es\nuna <u>serie formal</u> (i.e.,\n$cogrado(\\boldsymbol{b})\\geq{\\color{blue}{0}}$)\n$$X_t=\\sum_{j=0}^\\infty {b}_j U_{t-j};\\qquad\nt\\in\\mathbb{Z}$$ $\\;$ (pues cada $X_t$ es una suma de variables \"*del\npresente y pasado*\").\n\n"]},{"cell_type":"markdown","id":"dd5275fb-a204-42a3-a41b-765d6b4bf0b3","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La clase de procesos lineales incluye muchas e importantes subclases\nde procesos, algunas de las cuales son objeto principal de estudio de\neste curso.\n\n"]},{"cell_type":"markdown","id":"15ea8f7d-227e-4ce8-9355-76f956ddb94c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Media móvil infinita. MA($\\infty$)\n\n"]},{"cell_type":"markdown","id":"ca82dc88-ef92-4e89-ba1f-71ac55c08415","metadata":{},"source":["Sea $\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2)\\;$ y sea\n$\\;\\boldsymbol{\\psi}\\in \\ell^2\\;$ una serie formal con <u>infinitos\ntérminos NO nulos</u>; entonces el proceso estocástico\n$\\boldsymbol{\\psi}*\\boldsymbol{U}$, cuyos elementos son $$X_t\n\\;=\\;(\\boldsymbol{\\psi}*\\boldsymbol{U})_t\n\\;=\\;\\boldsymbol{\\psi}(B)U_t \\;=\\;\\sum_{j=0}^\\infty \\psi_j\nU_{t-j};\\qquad t\\in\\mathbb{Z}$$ se denomina proceso de *media móvil\ninfinita* MA($\\infty$).\n\n"]},{"cell_type":"markdown","id":"358dcf57-3421-45fe-a8a2-dbd17e540ea9","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Algunas clases de procesos lineales tienen una representación\nparsimoniosa, pues basta un número finito de parámetros para\nrepresentarlos completamente. Por ejemplo, cuando\n$\\boldsymbol{\\psi}$ tiene un número finito de términos no nulos&#x2026;\n\n"]},{"cell_type":"markdown","id":"629c0f7d-c8dd-431b-a830-745b1e28324f","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["#### Proceso de media móvil de orden $q$. MA($q$)\n\n"]},{"cell_type":"markdown","id":"99527991-4e5b-4ab5-b37f-5da69d089c23","metadata":{},"source":["Sea $\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2)\\;$ y sea\n$\\;\\boldsymbol{\\theta}\\;$ un <u>polinomio de grado $q$</u> con\n${\\color{#008000}{\\theta_{0}=1}}$; entonces el proceso estocástico\n$\\boldsymbol{\\theta}*\\boldsymbol{U}$, cuyos elementos son $$X_t\n\\;=\\;(\\boldsymbol{\\theta}*\\boldsymbol{U})_t\n\\;=\\;\\boldsymbol{\\theta}(B)U_t \\;=\\;\\sum_{j=0}^q\\theta_j\nU_{t-j};\\qquad t\\in\\mathbb{Z}$$ se denomina proceso de *media móvil*\nMA($q$).\n\n"]},{"cell_type":"markdown","id":"c4004621-83b2-453b-a9db-8f2998f81a33","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Es decir, si $\\;\\boldsymbol{\\theta}=(1-\\theta_1z-\\cdots-\\theta_qz^q)\\;$:\n$$ X_t = U_t - \\theta_1 U_{t-1} - \\cdots - \\theta_q U_{t-q}.$$\n\n"]},{"cell_type":"markdown","id":"4ec8653e-e125-4e79-ac3e-5ceb89cdb92e","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Hay otros procesos lineales con representación parsimoniosa.\n\n"]},{"cell_type":"markdown","id":"cf38f769-ac8c-464b-8c12-8a8b707a2e64","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["#### Proceso autorregresivo de orden $p$. AR($p$)\n\n"]},{"cell_type":"markdown","id":"c7b7ac92-d0ea-426e-9250-a78af4c97c87","metadata":{},"source":["Sea $\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2)\\;$, se denomina *proceso\nautorregresivo de orden $p$* a aquel proceso estocástico estacionario\n$\\;\\boldsymbol{X}\\;$ que es la solución de la siguiente ecuación en\ndiferencias $$\\boldsymbol{\\phi}*\\boldsymbol{X}=\\boldsymbol{U}$$ donde\n$\\;\\boldsymbol{\\phi}\\;$ un <u>polinomio de grado $p$</u> con ${\\color{#008000}{\\phi_{0}=1}}$;\n\n"]},{"cell_type":"markdown","id":"96073ba0-4799-484e-90cf-8f30289e0430","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por tanto, $$(\\boldsymbol{\\phi}*\\boldsymbol{X})_t=\n\\boldsymbol{\\phi}(\\mathsf{B})X_t= \\sum_{j=0}^p \\phi_j X_{t-j} = U_t.$$\n\n"]},{"cell_type":"markdown","id":"d12e75ac-bf19-40d9-9776-4a33c954946f","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Si $\\;\\boldsymbol{\\phi}=(1-\\phi_1z-\\cdots-\\phi_pz^p)\\;$ entonces\n$\\boldsymbol{X}=(X_t\\mid t\\in\\mathbb{Z})$ es solución de la ecuación:\n$$X_t + \\phi_1 X_{t-1} + \\cdots + \\phi_q X_{t-q} = U_t.$$\n\n"]},{"cell_type":"markdown","id":"eb2a90e8-168b-4abe-b786-9ce8ff2ed18b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema con la anterior definición es que la ecuación\n$\\boldsymbol{\\phi}*\\boldsymbol{X}=\\boldsymbol{U}$ no tiene solución\núnica (y en algunos casos ninguna solución es\nestacionaria). Despejemos $\\boldsymbol{X}$ para verlo.\n\nMultiplicando ambos lados de la ecuación por una inversa de\n$\\boldsymbol{\\phi}$ lo logramos:\n$$\\boldsymbol{X}=inversa(\\boldsymbol{\\phi})*\\boldsymbol{U}.$$ Y si\ndenotamos la secuencia $inversa(\\boldsymbol{\\phi})$ con\n$\\boldsymbol{a}$ entonces\n$$X_t=\\boldsymbol{a}(\\mathsf{B})U_t=\\sum_{j\\in\\mathbb{Z}} a_j\nU_{t-j}.$$\n\n"]},{"cell_type":"markdown","id":"8d0ad7d7-ac94-4f3a-8089-faf849268297","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Pero&#x2026; ¿Qué secuencia $\\boldsymbol{a}$ usamos como inversa de\n$\\boldsymbol{\\phi}$? Recuerde que hay infinitas y la mayoría no son\nsumables (si el polinomio $\\boldsymbol{\\phi}$ tiene raíces unitarias\nninguna lo es).\n\n<div class=\"org-center\">\n<p>\nEn tal caso la expresión\n$\\;\\boldsymbol{a}(\\mathsf{B})U_t=\\sum\\limits_{j=-\\infty}^\\infty a_j\nU_{t-j}\\;$ carece de sentido (pues no converge).\n</p>\n</div>\n\n"]},{"cell_type":"markdown","id":"b2bde4c8-a29d-4c06-bce0-8d744f10a09b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["**Requisitos** sobre el polinomio autorregresivo $\\boldsymbol{\\phi}$\n\n1.  Para tener un <u>proceso lineal</u>, exigiremos que $\\boldsymbol{\\phi}$\n    <u>no tenga raíces de módulo 1</u>.\n    \n    Entonces existe una única inversa absolutamente sumable: $\\boldsymbol{\\phi}^{-1} \\in\n       \\ell^1\\subset\\ell^2$.\n    \n    La inversa $\\boldsymbol{a}=\\boldsymbol{\\phi}^{-1}$ corresponde a la\n    única solución *estacionaria* de\n    $\\boldsymbol{\\phi}*\\boldsymbol{X}=\\boldsymbol{U}$.  (Si\n    $\\boldsymbol{\\phi}$ tuviera raíces de módulo 1 no existiría ni\n    $\\boldsymbol{\\phi}^{-1}$, ni la solución estacionaria).\n    \n    $$X_t=\\boldsymbol{\\phi}^{-1}(\\mathsf{B})U_t=\\sum_{j=-\\infty}^\\infty a_j U_{t-j}$$\n\n2.  Para tener un <u>proceso lineal causal</u> exigiremos que las raíces de\n    $\\boldsymbol{\\phi}$ sean mayores que 1 en valor absoluto (<u>raíces\n    fuera del círculo unidad</u>):\n    $\\boldsymbol{\\phi}^{-1}=\\boldsymbol{\\phi}^{-\\triangleright}$.\n    \n    $$X_t=\\boldsymbol{\\phi}^{-1}(\\mathsf{B})U_t=\\sum_{j=0}^\\infty a_j U_{t-j}$$\n\n"]},{"cell_type":"markdown","id":"5e98f129-0fd1-4fab-ad21-9aa0ace1d2b2","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El siguiente modelo lineal es una combinación (o generalización) de\nlos dos anteriores.\n\n"]},{"cell_type":"markdown","id":"24ec784e-21bb-4d62-8f42-46c5d74acf79","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Proceso autorregresivo de media móvil. ARMA($p,q$)\n\n"]},{"cell_type":"markdown","id":"428f6195-d66f-471a-95b4-87f5ad763333","metadata":{},"source":["Sea $\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2)\\;$, se denomina *proceso\nautorregresivo de media móvil $(p,q)$* al proceso estocástico\nestacionario $\\;\\boldsymbol{X}\\;$ que es la solución de la ecuación en\ndiferencias:\n$$\\boldsymbol{\\phi}*\\boldsymbol{X}=\\boldsymbol{\\theta}*\\boldsymbol{U}$$\ndonde el polinomio *autorregresivo* $\\;\\boldsymbol{\\phi}\\;$ tiene\n<u>grado $p$</u> con ${\\color{#008000}{\\phi_{0}=1}}$ y con todas sus raíces\nfuera del círculo unidad (*por los motivos anteriormente vistos*); y\nel polinomio *de media móvil* $\\;\\boldsymbol{\\theta}\\;$ es <u>de grado\n$q$</u> con ${\\color{#008000}{\\theta_{0}=1}}$; \n\n$$\\text{es decir,}\\qquad\n\\boldsymbol{X}=\\frac{\\boldsymbol{\\theta}}{\\boldsymbol{\\phi}}*\\boldsymbol{U};\n\\qquad\\text{donde}\\;\n\\frac{\\boldsymbol{\\theta}}{\\boldsymbol{\\phi}}\\equiv\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$$\n\n"]},{"cell_type":"markdown","id":"79aa06fc-3dee-45a9-9273-81751de08e0d","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Tanto $\\boldsymbol{\\phi}^{-1}$ como $\\boldsymbol{\\theta}$ son\nabsolutamente sumables y como $\\ell^1$ es un anillo,\n$\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}\\equiv\\frac{\\boldsymbol{\\theta}}{\\boldsymbol{\\phi}}\\in\\ell^1$\n(también es absolutamente sumable y por tanto de cuadrado sumable),\nconsecuentemente el proceso estocástico es un proceso lineal.\n$$X_t=\\frac{\\boldsymbol{\\theta}}{\\boldsymbol{\\phi}}(\\mathsf{B})U_t=\\sum_{j=0}^\\infty\na_j U_{t-j}$$ donde\n$\\boldsymbol{a}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$.\n\n"]},{"cell_type":"markdown","id":"199e896e-d26c-4900-8b7e-800fa8d4f981","metadata":{"slideshow":{"slide_type":"notes"}},"source":["#### Proceso autorregresivo de media móvil con media no nula\n\n"]},{"cell_type":"markdown","id":"0d047f31-87c2-4d15-a77a-382551eef913","metadata":{"slideshow":{"slide_type":"notes"}},"source":["Por último, consideremos un proceso $\\boldsymbol{Y}$ con media\ndistinta de cero, es decir, $$E(Y_t)=\\mu\\ne0$$ y definamos la\nsecuencia constante $\\boldsymbol{\\mu}=\\sum\\limits_{j\\in\\mathbb{Z}} \\mu\nz^j=(\\ldots,\\mu,\\mu,\\mu,\\ldots)$. \n\\medskip\n\nDecimos que $\\boldsymbol{Y}$ es un proceso ARMA($p,q$) con media\ndistinta de cero si $\\boldsymbol{X}$ es ARMA($p,q$)\n$$\\boldsymbol{\\phi}*\\boldsymbol{X}=\\boldsymbol{\\theta}*\\boldsymbol{U}$$\ndonde $\\boldsymbol{X}=\\boldsymbol{Y}-\\boldsymbol{\\mu}$ es\nevidentemente un proceso de media cero.  Por tanto\n\n\\begin{align*}\n\\boldsymbol{\\phi}*(\\boldsymbol{Y}-\\boldsymbol{\\mu})=&\\boldsymbol{\\theta}*\\boldsymbol{U}\\\\\n\\boldsymbol{\\phi}*\\boldsymbol{Y}-\\boldsymbol{\\phi}*\\boldsymbol{\\mu}=&\\boldsymbol{\\theta}*\\boldsymbol{U}\\\\\n\\boldsymbol{\\phi}*\\boldsymbol{Y}=&\\boldsymbol{\\phi}*\\boldsymbol{\\mu}+ \\boldsymbol{\\theta}*\\boldsymbol{U}\\\\\n\\end{align*}\n\nEs decir, si $\\boldsymbol{\\phi}(\\mathsf{B})$ es\n$\\;1-\\phi_1\\mathsf{B}-\\phi_2\\mathsf{B}^2-\\cdots-\\phi_p\\mathsf{B}^p,\\;$\nentonces $$\\boldsymbol{\\phi}(B){Y_t}=c+\\boldsymbol{\\theta}(B){U_t}$$\ndonde $$\\;c=(1-\\phi_1-\\phi_2-\\cdots-\\phi_p)\\mu\\;$$ y donde\n$\\;\\mu=E(Y_t)$, es un proceso autorregresivo de media móvil\nARMA($p,q$) *con media no nula*.\n\n"]},{"cell_type":"markdown","id":"bf15fd3c-0855-48d7-b5ab-bbd6e150dd47","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Primeros momentos de procesos estocásticos\n\n"]},{"cell_type":"markdown","id":"42eedfa1-d423-430a-a974-cd19094da617","metadata":{},"source":["Si $E(X_t)<\\infty$ para $t\\in\\mathbb{Z}$, entonces $E(\\boldsymbol{X})$ es\nla secuencia $$E(\\boldsymbol{X})=\\big(E(X_t)\\mid\nt\\in\\mathbb{Z}\\big)=\\sum\\nolimits_{t\\in\\mathbb{Z}} E(X_t)\nz^t=\\big(\\ldots,\\;E(X_{-1}),\\;E(X_{0}),\\;E(X_{1}),\\ldots\\big)$$\n\n"]},{"cell_type":"markdown","id":"cc441a79-9f59-48c2-bcd8-e51c6317e559","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Si $\\boldsymbol{Y}$ tiene segundos momentos finitos, la secuencia de\nautocovarianzas <u>de orden $k$</u> es\n\n\\begin{align*}\nCov(\\boldsymbol{Y},\\boldsymbol{Y}*z^k) = &\n% \\boldsymbol{\\gamma_k}= &\nE\\Big(\\big[\\boldsymbol{Y}-E(\\boldsymbol{Y})\\big]\\odot\\big[(\\boldsymbol{Y}-E(\\boldsymbol{Y}))*z^k\\big]\\Big)\\\\\n= & \n\\left.\\Big(E\\big[\\big(Y_t-E(Y_t)\\big)\\big(Y_{t-k}-E(Y_{t-k})\\big)\\big]\\; \\right| t\\in\\mathbb{Z}\\Big)\\\\\n=&\n% \\sum_{t\\in\\mathbb{Z}} \\gamma_{_{k,t}} z^t\n(\\gamma_{_{k,t}}\\mid t\\in\\mathbb{Z}) \\;=\\;\n(\\ldots,\\,\\gamma_{_{k,-1}},\\,{\\color{blue}{\\gamma_{_{k,0}}}},\\,\\gamma_{_{k,1}},\\,\\gamma_{_{k,2}},\\ldots);\\quad k\\in\\mathbb{Z}.\n\\end{align*}\n\n"]},{"cell_type":"markdown","id":"e1565a0d-ea30-4b26-b5cd-2749a8118abf","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Cuando $E(X_t)=\\mu$ para todo $t\\in\\mathbb{Z}$, entonces\n$E(\\boldsymbol{X})$ es una secuencia constante: $$E(\\boldsymbol{X}) =\n(\\mu\\mid t\\in\\mathbb{Z}) = \\mu\\boldsymbol{1}.$$\n\nSi $\\gamma_{_{k,t}}=\\gamma_k$ \npara todo $t\\in\\mathbb{Z}$, \nentonces definimos la <u>función de autocovarianzas</u> (todas):\n$$\\boldsymbol{\\gamma} = (\\gamma_{k}\\mid k\\in\\mathbb{Z}) =\n(\\ldots,\\,\\gamma_{-1},\\,{\\color{blue}{\\gamma_{0}}},\\,\\gamma_{1},\\,\\gamma_{2},\\ldots).$$\n\n"]},{"cell_type":"markdown","id":"bb6bd3ab-6ce3-4b63-acd1-d357cf24b4f8","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Esperanza y autocovarianzas de un proceso lineal\n\n"]},{"cell_type":"markdown","id":"6001c611-efbd-4310-a4b8-c0f3d745c196","metadata":{},"source":["Sea $\\;\\boldsymbol{X}=\\boldsymbol{\\psi}*\\boldsymbol{U},\\;$ donde\n$\\boldsymbol{\\psi}$ es una serie formal de cuadrado sumable\ncon cogrado $0$ y\n$\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2).\\quad$ Recordando que la\nconvolución es una operación lineal: $$E(\\boldsymbol{X})\n=E(\\boldsymbol{\\psi}*\\boldsymbol{U})\n=\\boldsymbol{\\psi}*E(\\boldsymbol{U})\n=\\boldsymbol{\\psi}*\\boldsymbol{0}=\\boldsymbol{0}.$$ Consecuentemente,\nla covarianza de orden $k$ para cada $X_t$ es\n\n\\begin{align*}\n\\gamma_{_{k,t}} = & E\\Big[\\big(\\boldsymbol{\\psi}(\\mathsf{B})X_t\\big)\\cdot \\big(\\boldsymbol{\\psi}(\\mathsf{B}) X_{t-k}\\big)\\Big] \n\\\\ = &\nE\\Big[\n (\\psi_0U_{t}+\\psi_1U_{t-1}+\\psi_2U_{t-2}\\cdots)\n (\\psi_0U_{t-k}+\\psi_1U_{t-k-1}+\\psi_2U_{t-k-2}\\cdots)\\Big]\n\\\\ = &\n\\sigma^2\\sum\\nolimits_{j\\in\\mathbb{Z}}\\psi_{j+k}\\psi_j\n\\qquad \\text{ ya que }\\; E(U_hU_j)=0\\; \\text{ si } \\;j\\ne h,\n\\end{align*}\n\nque no depende de $t$ ($\\boldsymbol{X}$ es estacionario). Es más, por\nla última ecuación de la lección 4 $$\\;\\gamma_k \\;=\\;\n\\sigma^2\\sum\\nolimits_{j\\in\\mathbb{Z}}\\psi_{j+k}\\psi_j \\;=\\;\n\\sigma^2\\big(\\boldsymbol{\\psi}(z)*\\boldsymbol{\\psi}(z^{-1})\\big)_k\n\\qquad \\text{ para } k\\in\\mathbb{Z}.$$ Y, por tanto\n\n\\begin{equation}\n \\label{eqAutoCovarianzaProcesoLineal}\n \\boldsymbol{\\gamma}=\\sigma^2\\boldsymbol{\\psi}(z)*\\boldsymbol{\\psi}(z^{-1})\n\\end{equation}\n\ncon grado igual al grado de $\\boldsymbol{\\psi}$ y cogrado igual a\nmenos el grado de $\\boldsymbol{\\psi}$.\n\n"]},{"cell_type":"markdown","id":"01fc16a2-3738-4459-89db-41b608b31b2a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### Covarianza cruzada entre dos procesos lineales causales\n\n"]},{"cell_type":"markdown","id":"2176b25f-e001-4eb8-8bfb-0c14c48fecc2","metadata":{},"source":["Sean $\\;\\boldsymbol{W}=\\boldsymbol{\\theta}*\\boldsymbol{U}\\quad$ e\n$\\quad\\boldsymbol{Y}=\\boldsymbol{\\psi}*\\boldsymbol{U},\\quad$ donde\n$\\boldsymbol{\\theta}$ y $\\boldsymbol{\\psi}$ son series formales de\ncuadrado sumable con cogrado $0$ y donde $\\;\\boldsymbol{U}\\sim\nWN(0,\\sigma^2)$.\n\nEntonces la covarianza cruzada (de orden $k\\in\\mathbb{Z}$) entre $W_t$\ne $Y_{t-k}$ es\n\n\\begin{align*}\nE\\Big[W_t\\cdot Y_{t-k}\\Big] = &\nE\\Big[\\big(\\boldsymbol{\\theta}(\\mathsf{B})U_t\\big)\\cdot \\big(\\boldsymbol{\\psi}(\\mathsf{B}) U_{t-k}\\big)\\Big] \n\\\\ = &\nE\\Big[\n (\\theta_0U_{t}+\\theta_1U_{t-1}+\\theta_2U_{t-2}\\cdots)\n (\\psi_0U_{t-k}+\\psi_1U_{t-k-1}+\\psi_2U_{t-k-2}\\cdots)\\Big]\n\\\\ = &\n\\sigma^2\\sum\\nolimits_{j\\in\\mathbb{Z}}\\theta_{j+k}\\psi_j\n\\qquad \\text{ ya que }\\; E(U_hU_j)=0\\; \\text{ si } \\;j\\ne h\n\\end{align*}\n\nque tampoco depende de $t$. Es más, por la última ecuación de la lección 4\n$$\\;\\gamma_{_{\\boldsymbol{W},\\boldsymbol{Y}}}(k) =\n\\sigma^2\\sum\\nolimits_{j\\in\\mathbb{Z}}\\theta_{j+k}\\psi_j =\n\\sigma^2\\big(\\boldsymbol{\\theta}(z)*\\boldsymbol{\\psi}(z^{-1})\\big)_k\\quad\n\\text{ para todo } k\\in\\mathbb{Z}.$$ Por tanto, la función de\ncovarianzas cruzadas es la secuencia\n\n\\begin{equation}\n \\label{eqCovarianzaCruzadaProcesosLineales}\n \\boldsymbol{\\gamma_{_{\\boldsymbol{W},\\boldsymbol{Y}}}} =\n \\sigma^2 \\boldsymbol{\\theta}(z)*\\boldsymbol{\\psi}(z^{-1})\n\\end{equation}\n\nde grado igual al grado de $\\boldsymbol{\\theta}$ y cogrado igual a menos\nel grado de $\\boldsymbol{\\psi}$.\n\n"]},{"cell_type":"markdown","id":"c9063825-6b32-4361-a634-60eb8ad018a7","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Las Ecuaciones de Yule-Walker para un AR($p$) estacionario\n\n"]},{"cell_type":"markdown","id":"3c42b7c6-8711-49c1-a197-2a5b945032c9","metadata":{},"source":["*Por una parte* (lado izquierdo):\n\nSi $\\boldsymbol{X}$ es un proceso (débilmente) estacionario con\n$E(\\boldsymbol{X})=\\boldsymbol{0}\\;$ y $\\;\\boldsymbol{\\phi}$ es una serie\nformal absolutamente sumable; entonces para $t,k\\in\\mathbb{Z}$\n\n\\begin{equation}\n  E\\Big[\\Big(\\boldsymbol{\\phi}(\\mathsf{B})X_t\\Big)\\cdot X_{t-k}\\Big]\n  \\quad = \\quad\n  \\boldsymbol{\\phi}(\\mathsf{B})E\\big(X_t\\cdot X_{t-k}\\big)\n  \\quad = \\quad\n  \\boldsymbol{\\phi}(\\mathsf{B})\\gamma_k\n  \\label{eqnLadoIzquierdoYW}\n\\end{equation}\n\nque no depende de $t$, por ser $\\boldsymbol{X}$ es un proceso\n(débilmente) estacionario.\n\n"]},{"cell_type":"markdown","id":"f0dd5e66-949e-4596-aaa7-e7fd49298b0e","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["*Por otra parte* (lado derecho):\n\nSi $\\boldsymbol{X}$ tiene representación\n$\\;\\boldsymbol{X}=\\boldsymbol{\\psi}*\\boldsymbol{U}$ donde\n$\\;\\boldsymbol{U}\\sim WN(0,\\sigma^2)$ y $\\boldsymbol{\\psi}$ es una\nserie formal de cuadrado sumable con $\\psi_0=1$; es decir\n\n$$\\quad X_t=U_t + \\sum\\nolimits_{j=1}^\\infty \\psi_j U_{t-j},$$\nentonces para $t,k\\in\\mathbb{Z}$\n\n\\begin{equation}\n  E[U_t\\cdot X_{t-k}] = E\\Big[U_t\\Big(U_{t-k} + \\sum\\nolimits_{j=1}^\\infty \\psi_j U_{t-k-j}\\Big) \\Big]=\n  \\begin{cases}\n  \\sigma^2 & \\text{cuando } k=0\\\\\n  0 & \\text{cuando } k\\ne0\n  \\end{cases}\n  \\label{eqnLadoDerechoYW}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"9e2b0828-7ce8-4120-9e16-32bdbb7f43f6","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea un AR($p$) estacionario:\n$\\;\\;\\boldsymbol{\\phi}(\\mathsf{B})X_t=U_t\\;\\;$ donde\n$\\;\\;\\boldsymbol{\\phi}(z)=1-\\phi_1z^1-\\cdots-\\phi_pz^p.\\;$\nMultiplicando por $X_{t-k}$ y tomando esperanzas:\n$$E\\Big[\\Big(\\boldsymbol{\\phi}(\\mathsf{B})X_t\\Big)\\cdot X_{t-k}\\Big] =\nE[U_t\\cdot X_{t-k}]$$\n\n**para $k=0$:** $\\quad$ (por $\\ref{eqnLadoIzquierdoYW}$ y $\\ref{eqnLadoDerechoYW}$)\n$$\\fbox{$\\boldsymbol{\\phi}(\\mathsf{B})\\gamma_0=\\sigma^2$}\n\\quad\\Rightarrow\\quad\n\\gamma_0-\\phi_1\\gamma_1-\\cdots-\\phi_p\\gamma_p=\\sigma^2\n\\quad\\Rightarrow\\quad \\sigma^2=\\gamma_0-\\sum\\nolimits_{j=1}^p\\phi_j\\gamma_j.$$\nDividiendo por $\\gamma_0$ (y recordando que $\\rho_0=1$):\n$$\\boldsymbol{\\phi}(\\mathsf{B})\\rho_0=\\frac{\\sigma^2}{\\gamma_0}\n\\quad\\Rightarrow\\quad\n\\fbox{$\\gamma_0=\\frac{\\sigma^2}{\\boldsymbol{\\phi}(\\mathsf{B})\\rho_0}$}\n\\quad\\Rightarrow\\quad\n\\gamma_0=\\frac{\\sigma^2}{1-\\sum\\nolimits_{j=1}^p\\phi_j\\rho_j}.$$ \n\n**para $k>0$:** $\\quad$ (por $\\ref{eqnLadoIzquierdoYW}$ y $\\ref{eqnLadoDerechoYW}$)\n$$\\fbox{$\\boldsymbol{\\phi}(\\mathsf{B})\\gamma_k=0$}\n\\quad\\Rightarrow\\quad\n\\gamma_k-\\phi_1\\gamma_{k-1}-\\cdots-\\phi_p\\gamma_{k-p}=0\n\\quad\\Rightarrow\\quad \\gamma_k=\\sum\\nolimits_{j=1}^p\\phi_j\\gamma_{k-j}.$$ \nDividiendo por $\\gamma_0$:\n$$\\fbox{$\\boldsymbol{\\phi}(\\mathsf{B})\\rho_k=0$}\n\\quad\\Rightarrow\\quad\n\\rho_k-\\phi_1\\rho_{k-1}-\\cdots-\\phi_p\\rho_{k-p}=0\n\\quad\\Rightarrow\\quad \\rho_k=\\sum\\nolimits_{j=1}^p\\phi_j\\rho_{k-j}.$$\n\n"]},{"cell_type":"markdown","id":"e1b1ab12-8a8a-4e9c-b868-a47ef5620311","metadata":{"slideshow":{"slide_type":"notes"}},"source":["Por tanto, la estructura autorregresiva del proceso impone que las\nautocovarianzas (y las autocorrelaciones) verifiquen las ecuaciones de\nYule-Walker.\n\n"]},{"cell_type":"markdown","id":"8accc7b3-479d-4526-a2e5-b1b3abc36e3e","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Esperanza y función de autocovarianzas para un ARMA($p,q$)\n\n"]},{"cell_type":"markdown","id":"572ec705-e6e1-46a3-9f03-5c8561ca4c5d","metadata":{},"source":["Sea un ARMA($p,q$) estacionario:\n$\\boldsymbol{\\phi}(\\mathsf{B}){X_t}=\\boldsymbol{\\theta}(\\mathsf{B}){U_t}\\;$\ndonde $\\boldsymbol{\\phi}$ y $\\boldsymbol{\\theta}$ no tienen raíces\ncomunes. Multiplicando por $X_{t-k}$, tomando esperanzas y\nsustituyendo $X_{t-k}$ por su representación MA($\\infty$), donde\n$\\boldsymbol{\\psi}=\\frac{\\boldsymbol{\\theta}}{\\boldsymbol{\\phi}}$:\n$$E\\Big[\\Big(\\boldsymbol{\\phi}(\\mathsf{B})X_t\\Big)\\cdot X_{t-k}\\Big] =\nE\\Big[\\Big(\\boldsymbol{\\theta}(\\mathsf{B})U_t\\Big)\\cdot X_{t-k}\\Big]\n\\;=\\; E\\Big[\\Big(\\boldsymbol{\\theta}(\\mathsf{B})U_t\\Big)\\cdot\n\\Big(\\boldsymbol{\\psi}(\\mathsf{B})U_{t-k}\\Big)\\Big]$$ Usando\n$\\eqref{eqnLadoIzquierdoYW}$, renombrando\n$\\;\\boldsymbol{\\theta}(\\mathsf{B})U_t=\\boldsymbol{W}\\;$ y\n$\\;\\boldsymbol{\\psi}(\\mathsf{B})U_t=\\boldsymbol{Y}:\\;$\n\n\\begin{align*}\n  \\boldsymbol{\\phi}(\\mathsf{B})\\gamma_k & = \\boldsymbol{\\gamma_{_{\\boldsymbol{W},\\boldsymbol{Y}}}}(k)\\\\\n  & =  \\sigma^2 \\Big(\\boldsymbol{\\theta}(z)*\\boldsymbol{\\psi}(z^{-1})\\Big)_k & \\text{por } \\eqref{eqCovarianzaCruzadaProcesosLineales}\n\\end{align*}\n\nY como $\\boldsymbol{\\theta}(z)*\\boldsymbol{\\psi}(z^{-1})$ tiene grado $q$ y cogrado $-\\infty$\n\n\\begin{equation}\n  \\boldsymbol{\\phi}(\\mathsf{B})\\gamma_k = \n  \\begin{cases}\n     0 & k > q\\; \\text{(como en un AR)}\\\\\n    \\sigma^2 \\Big(\\boldsymbol{\\theta}(z)*\\boldsymbol{\\psi}(z^{-1})\\Big)_k & k\\leq q\n  \\end{cases}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"1552184d-e493-48ba-9521-11bd2880bce0","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Un proceso de ruido blanco $\\boldsymbol{U}\\sim WN(0,\\sigma^2)$ es un\nproceso *débilmente estacionario* (o estacionario de segundo orden)\ntal que: $$E(U_t)=0;\\qquad t\\in\\mathbb{Z}$$ y $$\\boldsymbol{\\gamma} =\n(\\ldots,\\,0,\\,0,\\,{\\color{blue}{\\sigma^2}},\\,0,\\,0,\\ldots)=\\sigma^2\nz^0;$$ es decir, $\\;\\gamma_0=\\sigma^2\\;$ y $\\;\\gamma_k=0\\;$ para todo\n$k\\ne0$.\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.3.2"}},"nbformat":4,"nbformat_minor":5}