{"cells":[{"cell_type":"markdown","id":"ebf95a1e-78d4-4291-a959-5a5ff34c4b91","metadata":{},"source":"Econometría Aplicada. Lección 2\n===============================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"2a1460d2-5f35-490b-9337-ff3efb9c01d8","metadata":{},"source":["<div class=\"ABSTRACT\" id=\"org7b3012d\">\n<p>\nEn esta lección veremos regresión con series temporales y estimación\nde componentes (no observables) con modelos deterministas.\n</p>\n\n</div>\n\n-   [lección en html](https://mbujosab.github.io/EconometriaAplicada-SRC/Lecc02.html)\n-   [lección en mybinder](https://mybinder.org/v2/gh/mbujosab/EconometriaAplicada-SRC/HEAD?labpath=Lecc02.ipynb)\n\n"]},{"cell_type":"markdown","id":"f9ad25be-e876-4fa0-9935-bc5939256c43","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python\n\n"]},{"cell_type":"code","execution_count":1,"id":"f73b6dfb-33a2-440a-ba77-2deb662e6338","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Importamos algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # dataframe processing\nimport statsmodels.api as sm  # modelos estadísticos\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt # data visualization\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=True)\nmpl.rc('text.latex', preamble=r'\\usepackage{amsmath}')\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 15,5"]},{"cell_type":"code","execution_count":1,"id":"1863d16f-3dd9-4cbf-bcee-37f636588feb","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Usaré la siguiente función para transformar salidas en \\LaTeX{} de statsmodels a ficheros png \n# que incluiré en las transparencias\nfrom sympy.printing.preview import preview\ndef repr_png(tex, ImgFile):\n    preamble = \"\\\\documentclass[10pt,preview]{standalone}\\n\" \\\n        \"\\\\usepackage{booktabs,amsmath,amsfonts}\\\\begin{document}\"    \n    preview(tex, filename=ImgFile, viewer='file', preamble=preamble, dvioptions=['-D','250'])"]},{"cell_type":"markdown","id":"dad56b26-a7b2-44f6-a636-a6b5bc3ca9c1","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Lectura datos: Internat. airline passengers. Monthly totals in thousands. Jan 49 – Dec 60\n\n"]},{"cell_type":"code","execution_count":1,"id":"f7997c6c-c618-4204-9a01-84aa2579a7ea","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Leemos los datos de un fichero csv y generamos un dataframe de pandas cuyo índice es el tiempo\nOrigData = pd.read_csv('./database/Datasets-master/airline-passengers.csv')\nOrigData['Month'] = pd.to_datetime(OrigData['Month'])\nOrigData = OrigData.set_index(['Month'])\nprint(OrigData.head())"]},{"cell_type":"code","execution_count":1,"id":"0e5d9499-48dd-4903-b296-6b353dbe16b8","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Creamos un dataframe con el mismo índice temporal de los datos originales pero con los datos en logaritmos\nTransformedData = pd.DataFrame(index=OrigData.index)\nTransformedData['dataLog'] = np.log(OrigData['Passengers'])\nprint(TransformedData.head())"]},{"cell_type":"markdown","id":"e5afdb2c-f836-4a94-9b31-8ab933afa158","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Descomposición estructural de una serie temporal\n\n"]},{"cell_type":"markdown","id":"39f1a262-7850-415b-a180-250e32995bbe","metadata":{},"source":["En la lección anterior vimos que una estrategia para analizar series\ntemporales es transformar los datos para\n\n1.  primero lograr que sean \"***estacionarios***\" y\n2.  después, mediante más transformaciones, lograr una secuencia de\n    \"**datos *i.i.d***\" (este segundo paso aún no lo hemos abordado)\n\n(*recuerde que las expresiones \"datos estacionarios\" o \"datos i.i.d.\" son un abuso del lenguaje*).\n\n"]},{"cell_type":"markdown","id":"f168f877-811e-4f43-8e6b-eae27e4dde18","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Pero existe otro enfoque que pretende descomponer la serie temporal en\nlos siguientes componentes *\"no observables\"* (o en un subconjunto de\nellos):\n\n$$\\boldsymbol{y} = \\boldsymbol{t} + \\boldsymbol{c} + \\boldsymbol{s} + \\boldsymbol{e}$$\n\ndonde:\n\n-   **La tendencia \"$\\boldsymbol{t}$\":** recoge la lenta evolución de la\n    media a *largo plazo*.\n\n-   **El componente estacional \"$\\boldsymbol{s}$\":** recoge las\n    oscilaciones periódicas que se repiten regularmente en ciclos\n    estacionales (de año en año, o de semana en semana, etc.).\n\n-   **El componente cíclico \"$\\boldsymbol{c}$\":** Cuando aparece\n    explícitamente en el modelo, $\\boldsymbol{c}$ recoge las\n    oscilaciones a medio plazo. Es decir, aquellas de un plazo más largo\n    que las oscilaciones estacionales, pero más corto que la tendencia\n    de largo plazo. Si está ausente, dichas oscilaciones suelen aparecer\n    en el componente de la tendencia, que entonces también podemos\n    denominar *tendencia-ciclo*.\n\n-   **El componente irregular \"$\\boldsymbol{e}$\":** recoge las\n    oscilaciones no captadas por el resto de componentes, ya que debe\n    cumplir la siguiente identidad: $\\boldsymbol{e} = \\boldsymbol{y} -\n      \\boldsymbol{t} - \\boldsymbol{c} - \\boldsymbol{s}$.\n\nAjuste aceptable si (como poco) el componente irregular\n$\\boldsymbol{e}$ parece \"*estacionario*\".\n\n"]},{"cell_type":"markdown","id":"11d7e083-e1a9-491c-8795-99deec767ca5","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Tendencia determinista *lineal*\n\n"]},{"cell_type":"code","execution_count":1,"id":"22a2bde6-53d2-4cf1-8eaa-0b60445d50b0","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Ajustamos por MCO una tendencia linea. \n# Para ello, primero creamos un DataFrame con el regresando y los regresores del modelo\ndatosModelo1 = TransformedData[['dataLog']].copy()\nnsample = len(datosModelo1)\ndatosModelo1['cte'] = [1]*nsample\ndatosModelo1['time'] = np.linspace(1, nsample, nsample)\nmodel1 = sm.OLS(datosModelo1['dataLog'], datosModelo1[['cte', 'time']])\nresults1 = model1.fit()"]},{"cell_type":"code","execution_count":1,"id":"d1870261-d16d-4ad7-88d3-49d29e1b97d5","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["#Añadimos al DataFrame =datosModelo1= la tendencia ajustada, los residuos y la diferencia estacional de los residuos.\ndatosModelo1['yhat'] = datosModelo1['cte']*results1.params['cte']+datosModelo1['time']*results1.params['time']\ndatosModelo1['ehat'] = results1.resid\ndatosModelo1['ehatDiff12'] = datosModelo1['ehat'].diff(12)"]},{"cell_type":"code","execution_count":1,"id":"8c251517-7a8f-4d24-b776-9a123560c060","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Dibujamos los datos junto a la tendencia estimada\nplt.plot(datosModelo1['dataLog'])\nplt.plot(results1.fittedvalues)\nplt.grid()  \nplt.ylabel(r\"Log-Passengers, ($\\ln\\boldsymbol{x}$) \")"]},{"cell_type":"markdown","id":"594138f5-4e8f-48f3-92d9-3ad4e3c2ec34","metadata":{},"source":["El modelo de tendencia más simple es la recta de regresión (el\nregresor no constante es el índice $t$):\n\n$$\\ln{y_t}=\\underbrace{\\beta_1+\\beta_2\\cdot t}_{\\text{tendencia}} + e_t; \\quad t=1:114$$\n\n"]},{"cell_type":"markdown","id":"4973c24e-64b7-40da-a398-a86df0aed5a8","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["![img](./img/lecc02/airlinepass+linearTrend.png)\n\n$$\\widehat{\\ln{y_t}}=4.8137+0.01\\cdot\\big(t\\big), \\qquad t=1:114$$\n\n"]},{"cell_type":"code","execution_count":1,"id":"3a58780f-4f29-448d-bb07-737025623648","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["print(results1.summary())"]},{"cell_type":"markdown","id":"e171df5c-06d6-48c2-9b63-fc3c7d1d0d27","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["![img](./img/lecc02/resultsModel1.png)\n\n"]},{"cell_type":"markdown","id":"925fef74-57d7-4533-80ab-4700e586691e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["**<u>Componente irregular</u>**\n\n"]},{"cell_type":"code","execution_count":1,"id":"6c850a95-1446-47d3-99fe-776c44ff4516","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Gráfico de los residuos del ajuste.\nplt.grid()  \nplt.plot(results1.resid)"]},{"cell_type":"markdown","id":"174d6e05-90f2-4afc-86dc-f60d157299a4","metadata":{},"source":["![img](./img/lecc02/airlinepass+irreg.png)\n\nEn este caso, el modelo \n\n$$\\boldsymbol{y} = \\boldsymbol{t} + \\boldsymbol{e}$$\n\ndonde $\\boldsymbol{t}$ es una tendencia lineal no\nes un ajuste satisfactorio, pues el *componente irregular*\n$$\\boldsymbol{e}=\\boldsymbol{y}-\\boldsymbol{t}$$\nno tiene la apariencia de realización de un proceso estacionario.\n\n"]},{"cell_type":"code","execution_count":1,"id":"0ab68da8-1eb2-4673-9802-7cce2aca896b","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Gráfico de la diferencia estacional de los residuos del ajuste.\nplt.grid()  \nplt.plot(datosModelo1['ehatDiff12'])"]},{"cell_type":"markdown","id":"5e6c3029-c401-4a66-b066-f056765b6260","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Adicionalmente podemos ver que diferencia de orden 12 del componente\nirregular parece mostrar un componente cíclico con un periodo de unos\n4 años.\n\n![img](./img/lecc02/airlinepass+irregDiff12.png)\n\nEn el siguiente ejercicio probaremos con una tendencia cuadrática&#x2026;\n\n"]},{"cell_type":"markdown","id":"e1ea0875-d39e-4e7e-af01-5ac36958834a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Tendencia determinista *cuadrática*\n\n"]},{"cell_type":"code","execution_count":1,"id":"3e437643-4bfd-4d9e-a795-2482966c8d0b","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# creamos un DataFrame con el regresando y los regresores del modelo.\ndatosModelo2 = TransformedData[['dataLog']].copy()\nnsample = len(datosModelo1)\ndatosModelo2['cte'] = [1]*nsample\ndatosModelo2['time'] = np.linspace(1, nsample, nsample)\ndatosModelo2['sq_time'] = [t**2 for t in datosModelo2['time']]\n# Ajustamos por MCO una tendencia cuadrática a los datos.\nmodel2 = sm.OLS(datosModelo1['dataLog'], datosModelo2[['cte', 'time', 'sq_time']])\nresults2 = model2.fit()"]},{"cell_type":"code","execution_count":1,"id":"a4ce7453-7d99-49ae-815b-8a8a51a41a06","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Añadimos al DataFrame 'datosModelo2' la tendencia ajustada, los residuos y la diferencia estacional de los residuos.\ndatosModelo2['yhat'] = results2.fittedvalues\ndatosModelo2['ehat'] = results2.resid\ndatosModelo2['ehatDiff12'] = datosModelo2['ehat'].diff(12)"]},{"cell_type":"code","execution_count":1,"id":"e175bf82-809f-42b9-8025-0ea1df9e01e2","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Dibujamos los datos junto a la tendencia estimada.\nplt.plot(datosModelo1['dataLog'])\nplt.plot(results2.fittedvalues)\nplt.grid()  \nplt.ylabel(r\"Log-Passengers, ($\\ln\\boldsymbol{x}$) \")"]},{"cell_type":"markdown","id":"7460b6e4-699b-4bf0-b09f-d6920e0e91e0","metadata":{},"source":["$$\\ln{y_t}=\\underbrace{\\beta_1+\\beta_2\\cdot t + \\beta_3\\cdot t^2}_{\\text{tendencia}} + e_t; \\quad t=1:114$$\n\n"]},{"cell_type":"markdown","id":"f8048742-fd33-4205-8d49-3b6c1674911f","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["![img](./img/lecc02/airlinepass+quadraticTrend.png)\n\n$$\\widehat{\\ln{y_t}}=4.7364+(0.0132)\\cdot t +(-2.191e-05)\\cdot t^2, \\qquad t=1:114$$\n\n"]},{"cell_type":"code","execution_count":1,"id":"4020abac-ca0c-4922-93c6-20ec72dcf833","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["print(results2.summary())"]},{"cell_type":"markdown","id":"22c2612c-3f63-4309-8c25-2c1e1083087c","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["![img](./img/lecc02/resultsModel2.png)\n\n"]},{"cell_type":"markdown","id":"1d323f35-b4fd-44d7-9738-069c613fa319","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["**<u>Componente irregular</u>**\n\n"]},{"cell_type":"code","execution_count":1,"id":"d652cfa6-59e2-49d0-a667-312a1478e45d","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["plt.grid()  \nplt.plot(results2.resid)"]},{"cell_type":"markdown","id":"a577ce4b-a60c-443b-b3dc-765975f73a08","metadata":{},"source":["![img](./img/lecc02/airlinepass+irreg2.png)\n\nDe manera análoga al caso anterior, el modelo\n\n$$\\boldsymbol{y} = \\boldsymbol{t} + \\boldsymbol{e}$$\n\ndonde $\\boldsymbol{t}$ ahora es una *tendencia\ncuadrática* tampoco es un ajuste satisfactorio, pues el componente\nirregular $\\boldsymbol{e}$ sigue sin parecerse a la realización de un\nproceso estacionario.\n\n"]},{"cell_type":"code","execution_count":1,"id":"6856b387-bc54-4056-a4a8-c9ec859e589c","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["plt.grid()  \nplt.plot(datosModelo2['ehatDiff12'])"]},{"cell_type":"markdown","id":"870c46b0-274b-4ac2-b2e2-9ee70a5d28f2","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["También en este modelo la diferencia de orden 12 del componente\nirregular muestra un componente cíclico con un periodo de unos 4 años.\n\n![img](./img/lecc02/airlinepass+irregDiff12.png)\n\nPara obtener una *tendencia-ciclo* que capte este ciclo, son\nnecesarios procedimientos más sofisticados (por ejemplo TRAMO-SEATS, o\nX13-ARIMA, o STAMP, o LDHR, o E4, etc.) que estiman tendencias y\ncomponentes estacionales estocásticos.\n\n"]},{"cell_type":"markdown","id":"35a6b4de-3ee3-497d-8261-2dce55711191","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["En el siguiente ejercicio estimaremos un **componente estacional\ndeterminista** (junto a una tendencia cuadrática determinista).\n\n"]},{"cell_type":"markdown","id":"5ad219ce-1b0f-45fa-8d78-9cf2a6ce8d94","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Tendencia cuadrática más estacionalidad determinista mediante *dummies*\n\n"]},{"cell_type":"code","execution_count":1,"id":"97dafa87-b2ec-441e-8161-049cd8162e78","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Creamos un dataframe con los datos y los regresores 'cte', 't' y 't^2'\ndf = TransformedData[['dataLog']].copy()\nnsample = len(df)\ndf['cte']     = [1]*nsample\ndf['time']    = np.linspace(1, nsample, nsample)\ndf['sq_time'] = [t**2 for t in df['time']]"]},{"cell_type":"code","execution_count":1,"id":"06b6f9c9-bee8-4ab9-b97d-f7f0a765511b","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Creamos las /dummies/ estacionales\nfrom statsmodels.tsa.deterministic import Seasonality\nseas_gen = Seasonality(12, initial_period=1)\nseasonalDummies = seas_gen.in_sample(df.index)"]},{"cell_type":"code","execution_count":1,"id":"886dc8f9-0c6f-46a3-b05b-ea0779b52f30","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Creamos un dataframe con el regresando y todos los regresores del modelo\ndatosModelo3 = pd.concat([df, seasonalDummies],axis=1)\n# realizamos la regresión de la primera columna ('dataLog') sobre el resto de columnas del dataframe.\nmodel3 = sm.OLS(datosModelo3['dataLog'], datosModelo3.iloc[:,1:-1])\nresults3 = model3.fit()"]},{"cell_type":"code","execution_count":1,"id":"fbb35110-2d35-4db5-b67e-f88fcf096f8d","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# La combinación lineal de los regresores 'cte', 'time' y 'sq_time' usando los correspondientes\n# parámetros estimados nos da el componente de tendencia (determinista) estimado. \nTrendComp = datosModelo3[['cte','time','sq_time']].dot(results3.params[['cte','time','sq_time']])"]},{"cell_type":"code","execution_count":1,"id":"115064ee-1dd3-4430-bab8-017afc9c9dc4","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["rcParams['figure.figsize'] = 15,4\nplt.plot(datosModelo1['dataLog'])\nplt.plot(TrendComp)\nplt.grid()  \nplt.ylabel(r\"Log-Passengers, ($\\ln\\boldsymbol{x}$) \")"]},{"cell_type":"markdown","id":"f3ba5ac6-fc76-4e09-b841-cbb2ba24acfa","metadata":{},"source":["![img](./img/lecc02/airlinepass+TrendC.png)\n\n"]},{"cell_type":"code","execution_count":1,"id":"cb436ea7-a1ca-44d7-88de-b4a4788f62cc","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["SeasonalComp = (seasonalDummies.iloc[:,:-1]).dot(results3.params[3:])\nplt.grid()  \nplt.plot(SeasonalComp)"]},{"cell_type":"markdown","id":"e59f7eab-6757-45b0-bd8e-3e8ca9f05ac7","metadata":{},"source":["![img](./img/lecc02/airlinepass+SeasonalC.png)\n\n"]},{"cell_type":"markdown","id":"9c9c11c2-8b3e-4c5b-87ab-65f812635976","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Ajuste y componente irregular $\\boldsymbol{e}=\\boldsymbol{y}-\\boldsymbol{t}-\\boldsymbol{s}$\n\n"]},{"cell_type":"code","execution_count":1,"id":"16c6e9e7-d3c7-483f-8f68-125907820c18","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["plt.grid()  \nplt.plot(datosModelo3['dataLog'])\nplt.plot(TrendComp + SeasonalComp)"]},{"cell_type":"markdown","id":"2972843d-d537-426e-9846-6ebf7274297a","metadata":{},"source":["![img](./img/lecc02/airlinepass+yhat.png)\n\n"]},{"cell_type":"code","execution_count":1,"id":"0b29c26e-2cf8-4dd5-b359-6f56b37f4a29","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["plt.grid()  \nplt.plot(results3.resid)"]},{"cell_type":"markdown","id":"47803c13-cb81-4690-b7a9-cc2789e40a34","metadata":{},"source":["![img](./img/lecc02/airlinepass+IrregC.png)\n\n"]},{"cell_type":"markdown","id":"18e7ca91-0d83-4011-93f5-8db656516660","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Valoración de modelos con componentes deterministas\n\n"]},{"cell_type":"markdown","id":"f226f10d-959a-40d0-92b0-b19b5717aa86","metadata":{},"source":["-   Estos modelos resultan útiles para realizar un análisis descriptivo.\n\n-   Pero suelen funcionar bastante mal como herramienta de predicción:\n    -   no tienen en cuenta la dependencia inter-temporal de los datos (se\n        han estimado mediante una regresión como si los datos hubieran\n        sido de sección cruzada)\n    \n    -   Por ejemplo, a la hora de prever el dato de enero de 1961, en este\n        modelo pesa tanto el dato de enero de 1949 como el dato de enero\n        de 1960.\n\nEn general, para que los modelos funcionen bien en predicción deben\n*dar un mayor peso a los datos recientes* frente a los datos alejados\nen el tiempo.\n\nPero sigamos explorando este modelo&#x2026;\n\n"]},{"cell_type":"markdown","id":"c3fcbeea-09cd-426b-aead-b51b6b4abaa7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["**Hay parámetros no significativos&#x2026;** (p-valores para dummies enero,\nfebrero y octubre).\n\n"]},{"cell_type":"code","execution_count":1,"id":"20a51104-419c-4cb3-bf23-9eaf3cc41557","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["repr_png(results3.summary().as_latex(), \"./img/lecc02/resultsModel3.png\")"]},{"cell_type":"markdown","id":"f4a53a9b-e903-4faf-8e7a-7919d9237ead","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["<div>\n<img src=\"./img/lecc02/resultsModel3.png\" width=\"400\" class=\"center\"/>\n</div>\n\n"]},{"cell_type":"markdown","id":"a5643f6a-ca22-4daa-89e7-650fd59a828c","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["**podemos eliminarlos secuencialmente** (quitando cada vez la variable de mayor p-valor)\n\n"]},{"cell_type":"code","execution_count":1,"id":"44d1c127-e5b2-4eb8-aa95-98bbf86fffef","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import operator\ndef remove_most_insignificant(df, results):\n    # use operator to find the key which belongs to the maximum value in the dictionary:\n    max_p_value = max(results.pvalues.items(), key=operator.itemgetter(1))[0]\n    # this is the feature you want to drop:\n    df.drop(columns = max_p_value, inplace = True)\n    return df"]},{"cell_type":"code","execution_count":1,"id":"6418b8c6-d22e-40fb-846a-7d45e26ada40","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["y = datosModelo3['dataLog']\nX = datosModelo3.iloc[:,1:-1]\nsignificacion = 0.05\ninsignificant_feature = True\nwhile insignificant_feature:\n        model4 = sm.OLS(y, X)\n        results4 = model4.fit()\n        significant = [p_value < significacion for p_value in results4.pvalues]\n        if all(significant):\n            insignificant_feature = False\n        else:\n            if X.shape[1] == 1:  # if there's only one insignificant variable left\n                print('No significant features found')\n                results4 = None\n                insignificant_feature = False\n            else:            \n                X = remove_most_insignificant(X, results4)\n\nprint(results4.summary())"]},{"cell_type":"markdown","id":"73063902-983c-4240-8c56-bcf749e87b98","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["<div>\n<img src=\"./img/lecc02/resultsModel4.png\" width=\"400\" class=\"center\"/>\n</div>\n\n\n\nPero esta inferencia es incorrecta. Con auto-correlación la varianza\ndel estimador MCO es diferente (**la estimación por defecto de las\ndesviaciones típicas es incorrecta**)\n\n"]},{"cell_type":"markdown","id":"8b75380a-f5ec-4700-bcef-87695a914a09","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Perturbaciones no esféricas\n\n"]},{"cell_type":"markdown","id":"5d3bf5e1-300b-4ead-a74a-3955fe8ee2f5","metadata":{},"source":["Considere el modelo\n$\\boldsymbol{y}=\\boldsymbol{\\mathsf{X}\\beta}+\\boldsymbol{U}.\\;$ Bajo\nlos supuestos habituales\n\n$$E(\\boldsymbol{U}\\mid\\boldsymbol{\\mathsf{X}})=\\boldsymbol{0},\\quad\nVar(\\boldsymbol{U}\\mid\\boldsymbol{\\mathsf{X}})=\\sigma^2\\boldsymbol{\\mathsf{I}}\\quad\n\\text{y} \\quad E(\\boldsymbol{\\mathsf{X'X}}) \\text{ es invertible}$$\n\nel estimador\n$\\;\\widehat{\\boldsymbol{\\beta}}=(\\boldsymbol{\\mathsf{X'X}})^{-1}\\boldsymbol{\\mathsf{X'}Y}\\;$\nes insesgado y eficiente, con varianza\n\n$$\\;Var(\\widehat{\\boldsymbol{\\beta}}\\mid\\boldsymbol{\\mathsf{X}})=\\sigma^2(\\boldsymbol{\\mathsf{X'X}})^{-1}$$\n\n"]},{"cell_type":"markdown","id":"8567bfc5-7191-4071-a39a-6bf42b38fc32","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Pero si las perturbaciones $\\boldsymbol{U}$ del modelo son\nheterocedásticas y/o autocorreladas\n$$Var(\\boldsymbol{U}\\mid\\boldsymbol{\\mathsf{X}})=\\boldsymbol{\\Sigma}\\ne\\sigma^2\\boldsymbol{\\mathsf{I}}$$\nentonces el estimador $\\widehat{\\boldsymbol{\\beta}}$, aunque\ninsesgado, ya no es eficiente; y su varianza es\n\n$$Var(\\widehat{\\boldsymbol{\\beta}}\\mid\\boldsymbol{\\mathsf{X}})=Var(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\mathsf{I}}\\boldsymbol{\\beta}\\mid\\boldsymbol{\\mathsf{X}})=\n(\\boldsymbol{\\mathsf{X'X}})^{-1}\\boldsymbol{\\mathsf{X'}}\n\\boldsymbol{\\Sigma}\n\\boldsymbol{\\mathsf{X}}(\\boldsymbol{\\mathsf{X'X}})^{-1}.$$\n\n"]},{"cell_type":"markdown","id":"cd27ada8-00f4-464b-943f-f666a0d1a183","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Test de autocorrelación de Breusch y Godfrey\n\n"]},{"cell_type":"markdown","id":"8aedc722-d17c-4224-af74-65a455c01e59","metadata":{},"source":["El tests el de Breusch y Godfrey (y el de Durbin-Watson) contrastan la $H_0$ de *no autocorrelación*.\n\n"]},{"cell_type":"markdown","id":"5f5dd3b7-4575-493c-aac4-4480d0e29f0c","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["\nConsidere el *modelo de regresión lineal* \n\n\\begin{equation}\n\\label{orgda12e5d}\nY_t = \\beta_1+ \\beta_2 X_{t,1} + \\cdots +  \\beta_k X_{t,k+1} + U_t \n\\end{equation}\n\n\ndonde las perturbaciones $\\boldsymbol{U}$ quizá siguen un esquema\nauto-regresivo $AR(p)$:\n\n$$U_t = \\rho_1 U_{t-1} + \\rho_2 U_{t-2}  + \\cdots + \\rho_p U_{t-p} + \\varepsilon_t$$\n\n-   **Paso 1**. Obtener los errores $\\hat{\\boldsymbol{e}}$ del ajuste MCO\n    del modelo ([1](#orgda12e5d)) con una muestra de tamaño $T$.\n-   **Paso 2**. Calcular el $R^2$ de la *regresión auxiliar* de los\n    errores $\\hat{\\boldsymbol{e}}$ sobre los regresores del modelo\n    original ([1](#orgda12e5d)) y sobre los $p$ primeros retardos\n    de $\\hat{\\boldsymbol{e}}$.  $$\\hat{e}_t = \\alpha_0 + \\alpha_1\n      X_{t,1} + \\cdots \\alpha_k X_{t,k} + \\rho_1 \\hat{e}_{t-1} + \\rho_2\n      \\hat{e}_{t-2} + \\cdots + \\rho_p \\hat{e}_{t-p} + \\varepsilon_t$$\n\nAsintóticamente y bajo la $H_0$ de *no autocorrelación*: $\\quad\\rho_i = 0\\text{ para todo }i$\n\n$$n R^2\\,\\sim\\,\\chi^2_p,$$\n\n\ndonde $R^2$ es el coeficiente de determinación de la regresión\nauxiliar y $n=T-p$.\n\n"]},{"cell_type":"markdown","id":"09fa4774-516a-43f7-a5bf-c1e80c319514","metadata":{"slideshow":{"slide_type":"notes"}},"source":["**El test de Durbin-Watson** contrasta la autocorrelación <u>de orden\nuno</u>. Para muestras grandes, el test es aproximadamente igual a\n$2(1-{\\hat {\\rho }})$, donde ${\\hat{\\rho}}$ es la autocorrelación de\norden uno de los residuos. Por tanto, valores del test próximos a 2\nindican no autocorrelación, valores próximos a 0 indican fuerte\nautocorrelación positiva y valores próximos a 4 indican fuerte\nautocorrelación negativa.\n\n"]},{"cell_type":"code","execution_count":1,"id":"1cab0f17-899b-4d09-99de-29378ee4b1af","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import statsmodels.stats.diagnostic as dg\n#perform Breusch-Godfrey test of order p = 3\narbg = dg.acorr_breusch_godfrey(results4, nlags=3, store=True)\narbg[:1]\nrepr_png(arbg[-1].resols.summary().as_latex(), \"./img/lecc02/resultsBreusch-Godfrey.png\")"]},{"cell_type":"markdown","id":"27679472-092e-471b-bb7f-6ddb7f7cef14","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   Valor del estadístico: $\\quad 62.7119\\qquad$ (p-valor: $\\; 1.55e-13$)\n-   $x_{12}$ corresponde al primer retardo en la regresión auxiliar y es muy significativo\n\n<div>\n<img src=\"./img/lecc02/resultsBreusch-Godfrey.png\" width=\"450\" class=\"center\"/>\n</div>\n\n"]},{"cell_type":"markdown","id":"c6d3396b-6de2-4681-bb29-3cbe04bbb19c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Errores estándar robustos\n\n"]},{"cell_type":"markdown","id":"79b15fee-e353-4a25-b21a-d17c075fd0a6","metadata":{},"source":["Un procedimiento adecuado en presencia de autocorrelación y muestras\ngrandes consiste en usar errores estándar *robustos* (**HAC** -\nheteroscedasticity and autocorrelation robust covariance matrix) al\nrealizar inferencia con la estimación de los parámetros.\n\n1.  las estimaciones serán insesgadas, consistentes pero ineficientes,\n\n2.  los residuos son los mismos y, por tanto, estarán autocorrelados, aunque\n\n3.  la inferencia a partir de errores estándar robustos será válida\n\n"]},{"cell_type":"code","execution_count":1,"id":"3ad73f16-6299-4d1b-aa8e-56e57207e3c3","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["y = datosModelo3['dataLog']\nX = datosModelo3.iloc[:,1:-1]\nmodel5 = sm.OLS(y, X)\nresults5 = model5.fit()\nprint(results5.get_robustcov_results(cov_type='HAC', maxlags=3, use_correction=True).summary())"]},{"cell_type":"code","execution_count":1,"id":"1c7b3d81-f1f3-4fa1-bcc0-0809a99068e1","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["repr_png(results5.get_robustcov_results(cov_type='HAC', maxlags=3, use_correction=True).summary().as_latex(), \"./img/lecc02/resultsModel5.png\")"]},{"cell_type":"markdown","id":"af0633bb-ef06-4308-b782-fb03c0cbcc81","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Covariance type**: HAC (heteroscedasticity and autocorrelation robust covariance matrix)\n\n<div>\n<img src=\"./img/lecc02/resultsModel5.png\" width=\"400\" class=\"center\"/>\n</div>\n\nAhora, y empleando errores estándar robustos (HAC), podemos reducir el\nmodelo de manera más cuidadosa usando desviaciones típicas\nrobustas. El modelo reducido es&#x2026;\n\n"]},{"cell_type":"code","execution_count":1,"id":"dc4f8bd5-89cb-4854-bcf1-9fc58bbeba13","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["y = datosModelo3['dataLog']\nX = datosModelo3.iloc[:,1:-1]\n\nsignificacion = 0.05\n\ninsignificant_feature = True\nwhile insignificant_feature:\n        results6      = sm.OLS(y, X).fit()\n        robustResults = results6.get_robustcov_results(cov_type='HAC', maxlags=3, use_correction=True)\n        robustPvalues = pd.Series(index=results6.pvalues.index, data=robustResults.pvalues)\n\n        significant = [p_value < significacion for p_value in robustPvalues]\n\n        \n        if all(significant):\n            insignificant_feature = False\n        else:\n            if X.shape[1] == 1:  # if there's only one insignificant variable left\n                print('No significant features found')\n                results6 = None\n                insignificant_feature = False\n            else:            \n                X = remove_most_insignificant(X, results6)\nprint(robustResults.summary())\nrepr_png(robustResults.summary().as_latex(), \"./img/lecc02/resultsModel6.png\")"]},{"cell_type":"markdown","id":"068a439b-c726-439c-bf0e-d00fcddf698b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["<div>\n<img src=\"./img/lecc02/resultsModel6.png\" width=\"400\" class=\"center\"/>\n</div>\n\n-   Nótese que ahora (HAC) se aprecia que enero y octubre son significativos al 5%\n-   Pero la estimación MCO no es eficiente en presencia de auto-correlación\n\n"]},{"cell_type":"markdown","id":"8aa300ad-2c32-4e29-9d62-e47c015ac51d","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Modelo del error\n\n"]},{"cell_type":"markdown","id":"2d90ad5c-b07d-4d78-b63b-966d02c87624","metadata":{},"source":["En el modelo\n$\\boldsymbol{y}=\\boldsymbol{\\mathsf{X}\\beta}+\\boldsymbol{U},\\;$ si las\nperturbaciones presentan heterocedasticidad y/o auto-correlación, y\npor tanto\n$$Var(\\boldsymbol{U}\\mid\\boldsymbol{\\mathsf{X}})=\\boldsymbol{\\Sigma}\\ne\\sigma^2\\boldsymbol{\\mathsf{I}},$$\nel Teorema de Gauss-Markov ya no es válido, ya que es posible explotar\nla estructura de la matriz $\\boldsymbol{\\Sigma}$ para minimizar la\nvarianza del estimador.\n\nEn particular, el estimador lineal de mínima varianza es el estimador\nMCG (mínimos cuadrados generalizados)\n\n$$\\;\\widehat{\\boldsymbol{\\beta}}=(\\boldsymbol{\\mathsf{X'}}\\boldsymbol{\\mathsf{\\Sigma}}^{-1}\\boldsymbol{\\mathsf{X}})^{-1}\\boldsymbol{\\mathsf{X'}}\\boldsymbol{\\mathsf{\\Sigma}}^{-1}\\boldsymbol{y}\\;$$\n\nEl problema es que, en general, la matriz $\\boldsymbol{\\Sigma}$ es\ndesconocida.\n\nUna solución es aplicar un procedimiento iterativo en el que se estima\nla matriz $\\boldsymbol{\\Sigma}$ empleando los errores del ajuste de\nuna primera regresión. Con dicha matriz\n$\\widehat{\\boldsymbol{\\Sigma}}$ se re-estima el modelo por MCG&#x2026; con\nlos nuevos errores se re-estima $\\boldsymbol{\\Sigma}$&#x2026; y vuelta a\nempezar&#x2026;\n\nEl algoritmo se detiene cuando las estimaciones convergen a valores\nestables.\n\n"]},{"cell_type":"markdown","id":"9183dbdb-f243-4233-8cd8-a9c94952faf7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Cuando realizamos el Test de Breusch-Godfrey vimos que en la regresión\nauxiliar el primer retardo de los errores era significativo. Por\ntanto, vamos a indicar que las perturbaciones siguen un proceso AR(1).\nEl decir, vamos a estimar el modelo\n\n$$\\ln{y_t}=\\underbrace{\\beta_1+\\beta_2\\cdot t+\\beta_3\\cdot t^2}_{\\text{tendencia}} + \\underbrace{\\alpha_1 S_{t1} + \\alpha_3 S_{t3} + \\cdots + \\alpha_11 S_{t11}}_{\\text{comp. estacional}} + \\epsilon_t$$\n\ndonde las perturbaciones $\\boldsymbol{\\epsilon}=\\{\\epsilon_t\\}$ siguen\nel modelo\n\n$$\\epsilon_t = \\rho_1 \\epsilon_{t-1} + e_t$$\n\n(*en este caso la estimación (**GLSAR**) converge en 7 iteraciones*)\n\n"]},{"cell_type":"code","execution_count":1,"id":"1b45adf6-b194-4648-ae4e-31935c3cd094","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["model = sm.GLSAR(y, X, rho=1) # rho=1 indica autocorrelación de orden uno\nfor i in range(7):\n    results = model.fit()\n    print(\"AR coefficients: {0}\".format(model.rho))\n    rho, sigma = sm.regression.yule_walker(results.resid,\n                                           order=model.order)\n    model = sm.GLSAR(y, X, rho)"]},{"cell_type":"code","execution_count":1,"id":"bff90c06-12e7-4542-aed0-10c96581b46d","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["print(results.summary())"]},{"cell_type":"markdown","id":"2cc5f72c-799a-4b71-adf7-0242ba3bade5","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["<div>\n<img src=\"./img/lecc02/resultsModel7.png\" width=\"600\" class=\"center\"/>\n</div>\n\n"]},{"cell_type":"code","execution_count":1,"id":"19bee7e9-6f53-4d53-8389-4b01c621091a","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# este código realiza las mismas iteraciones que bloque de código de más arriba\nmodel2 = sm.GLSAR(y, X, rho=1)\nres = model2.iterative_fit(maxiter=7)\nmodel2.rho\nprint(model2.fit().summary())"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}